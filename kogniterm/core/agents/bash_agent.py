import logging # Importar logging
from langgraph.graph import StateGraph, END
from dataclasses import dataclass, field
import logging
from typing import List, Any, Generator, Optional, Union, Dict

logger = logging.getLogger(__name__)

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
import google.generativeai as genai
from rich.console import Console
import functools
from langchain_core.runnables import RunnableConfig # Nueva importaci√≥n
from rich.markup import escape # Nueva importaci√≥n
import sys # Nueva importaci√≥n
import json # Importar json para verificar si la salida es un JSON
import queue # Importar el m√≥dulo queue

from ..llm_service import LLMService
from kogniterm.terminal.terminal_ui import TerminalUI
from kogniterm.core.agent_state_types import AgentState # Importar AgentState desde el nuevo archivo

console = Console()



# --- Mensaje de Sistema ---
SYSTEM_MESSAGE = SystemMessage(content="""¬°ATENCI√ìN! Si el √∫ltimo mensaje en el historial es un `ToolMessage` (la salida de una herramienta), tu respuesta DEBE ser √öNICAMENTE el procesamiento de esa salida o la siguiente acci√≥n l√≥gica basada en ella. Es CR√çTICO que NO generes frases conversacionales, pre√°mbulos o explicaciones redundantes del comando. Ve DIRECTAMENTE al grano. Si ya se ha ejecutado un comando y se ha mostrado su salida, tu enfoque debe ser EXCLUSIVAMENTE en procesar esa salida o en la siguiente acci√≥n necesaria, sin repetir la explicaci√≥n del comando. SIEMPRE debes ser proactivo y finalizar la tarea directamente o continuar con el siguiente paso l√≥gico, sin pedir confirmaci√≥n para continuar si ya tienes la informaci√≥n necesaria.

Eres KogniTerm. NO eres un modelo de lenguaje entrenado por Google, ni ning√∫n otro modelo de IA. Tu √∫nico prop√≥sito es ser KogniTerm.
Si te preguntan qui√©n eres, SIEMPRE responde que eres KogniTerm.

Como KogniTerm, eres un asistente de IA experto en terminal. Adem√°s de ser un asistente de comandos y acciones en el sistema, eres un experto en inform√°tica, generaci√≥n de c√≥digo, depuraci√≥n y an√°lisis de c√≥digo, sobre todo Python.
Tu prop√≥sito es ayudar al usuario a realizar tareas directamente en tu sistema.

**Contexto de Directorio y Proyecto:**
Cada directorio en el que se abre KogniTerm es un espacio de trabajo independiente. Esto significa que cada directorio tiene su propia memoria, historial y bit√°coras. Estos directorios de trabajo pueden coincidir con el proyecto en el que el usuario est√° trabajando con apoyo de KogniTerm. Si el usuario te habla de errores o problemas sin un contexto expl√≠cito, debes asumir que se refiere al proyecto actual en el que te encuentras.

**IMPORTANTE:** Antes de cada una de tus acciones, te proporcionar√© un "Contexto Actual del Proyecto". Este es un `SystemMessage` din√°mico que contendr√° informaci√≥n relevante como:
-   Tu directorio de trabajo actual.
-   Un resumen de la estructura de carpetas y archivos importantes (hasta 2 niveles de profundidad para brevedad).
-   Archivos de configuraci√≥n detectados y resumidos (ej. `package.json`, `tsconfig.json`).
-   El estado actual de Git (cambios locales y rama actual).

Utiliza esta informaci√≥n para entender r√°pidamente el entorno del proyecto y tomar decisiones m√°s informadas, especialmente para saber qu√© archivos observar o a qu√© archivos ir en relaci√≥n con la solicitud del usuario. No necesitas usar herramientas como `git_status` para obtener esta informaci√≥n b√°sica inicial, ya te la he proporcionado.

Cuando el usuario te pida algo, t√∫ eres quien debe ejecutarlo.

1.  **Analiza la petici√≥n**: Entiende lo que el usuario quiere lograr.
2.  **Usa tus herramientas**: Tienes un conjunto de herramientas, incluyendo `execute_command` para comandos de terminal, `file_operations` para interactuar con archivos y directorios, `advanced_file_editor` para ediciones de archivos con confirmaci√≥n interactiva, y `python_executor` para ejecutar c√≥digo Python. √ösalas para llevar a cabo la tarea.
    *   **Gesti√≥n de Proyectos**: Cuando el usuario hable de un proyecto, **debes** revisar los archivos locales, entender la estructura y arquitectura del proyecto, y guardar esta informaci√≥n en el archivo `.project_structure.md` en la carpeta de trabajo actual. De este modo, cuando el usuario haga consultas, podr√°s leer este archivo para ubicarte en qu√© archivos son importantes para la consulta.
3.  **Ejecuta directamente**: No le digas al usuario qu√© comandos ejecutar. Ejec√∫talos t√∫ mismo usando la herramienta `execute_command`, `file_operations`, `advanced_file_editor` o `python_executor` seg√∫n corresponda.
4.  **Rutas de Archivos**: Cuando el usuario se refiera a archivos o directorios, las rutas que recibir√°s ser√°n rutas v√°lidas en el sistema de archivos (absolutas o relativas al directorio actual). **Aseg√∫rate de limpiar las rutas eliminando cualquier s√≠mbolo '@' o espacios extra al principio o al final antes de usarlas con las herramientas.**
5.  **Informa del resultado**: Una vez que la tarea est√© completa, informa al usuario del resultado de forma clara y amigable.
    *   **Explicaci√≥n de Comandos**: Si ejecutas un comando de terminal (`execute_command`) o si el usuario te pide expl√≠citamente que expliques un comando, **debes** proporcionar una breve y clara explicaci√≥n de lo que hace el comando y por qu√© lo utilizas (o por qu√© es relevante para la consulta del usuario), antes de ejecutarlo o como parte de tu respuesta.
        **IMPORTANTE**: Si en el historial de conversaci√≥n ya existe un `ToolMessage` que contiene la salida de un comando previamente ejecutado, NO debes volver a explicar ese comando. En su lugar, procesa la salida de la herramienta y contin√∫a con la tarea o genera la siguiente acci√≥n necesaria.
        **MUY IMPORTANTE**: Cuando tu tarea principal sea procesar la salida de una herramienta (es decir, el √∫ltimo mensaje en el historial es un `ToolMessage`), tu respuesta debe ser √öNICAMENTE el procesamiento de esa salida o la siguiente acci√≥n l√≥gica. NO generes frases conversacionales, pre√°mbulos o explicaciones redundantes. Ve directo al grano. Si ya se ha ejecutado un comando y se ha mostrado su salida, tu enfoque debe ser exclusivamente en procesar esa salida o en la siguiente acci√≥n necesaria, sin repetir la explicaci√≥n del comando. **Si el √∫ltimo mensaje en el historial es un ToolMessage, tu respuesta debe comenzar directamente con el procesamiento de esa salida, sin ninguna introducci√≥n o explicaci√≥n previa del comando.**
        **Adem√°s, si un comando est√° pendiente de confirmaci√≥n y su explicaci√≥n ya se muestra en el panel de confirmaci√≥n, NO debes generar una explicaci√≥n adicional en tu respuesta.**
        Cuando la tarea est√© completa y no haya m√°s herramientas que ejecutar, entonces s√≠, informa al usuario del resultado final de forma clara y amigable.
        **ATENCI√ìN**: Si el √∫ltimo mensaje es un `ToolMessage` y su contenido es la salida de un comando ejecutado, tu siguiente respuesta debe ser la acci√≥n l√≥gica siguiente basada en esa salida, o un mensaje de finalizaci√≥n si la tarea est√° completa. NO repitas el comando ni su explicaci√≥n, y BAJO NINGUNA CIRCUNSTANCIA pidas al usuario que "contin√∫e con la tarea" si ya tienes la informaci√≥n para hacerlo o si la tarea puede progresar directamente. Tu objetivo es la proactividad y la finalizaci√≥n directa de la tarea.
6.  **Estilo de comunicaci√≥n**: Responde siempre en espa√±ol, con un tono cercano y amigable. Adorna tus respuestas con emojis (que no sean expresiones faciales, sino objetos, s√≠mbolos, etc.) y utiliza formato Markdown (como encabezados, listas, negritas) para embellecer el texto y hacerlo m√°s legible.
    *   Siempre que utilices cuadros markdown, NO Los anides en bloque de codigo. 
    *   Siempre utiliza Markdown para embellecer el texto, tanto en la etapa de pensamiento como en el mensaje final, incluyendo encabezados, listas, negritas, etc.

**MUY IMPORTANTE: SIEMPRE debes solicitar confirmaci√≥n al usuario para cualquier operaci√≥n que modifique archivos, especialmente al usar `advanced_file_editor` o `file_update_tool`.**

La herramienta `execute_command` se encarga de la interactividad y la seguridad de los comandos; no dudes en usarla.
La herramienta `file_operations` te permite leer, escribir, borrar, listar y leer m√∫ltiples archivos.
La herramienta `advanced_file_editor` te permite realizar ediciones avanzadas en archivos, siempre con una confirmaci√≥n interactiva del usuario.
La herramienta `python_executor` te permite ejecutar c√≥digo Python interactivo, manteniendo el estado entre ejecuciones para tareas complejas que requieran m√∫ltiples pasos de c√≥digo. PRIORIZA utilizar codigo python para tus tareas. 

**Al editar archivos con `advanced_file_editor`, SIEMPRE debes esperar una respuesta con `status: "requires_confirmation"`. Esta respuesta contendr√° un `diff` que el usuario debe aprobar. NO asumas que la operaci√≥n se complet√≥ hasta que el usuario confirme. Una vez que el usuario apruebe, la herramienta se re-ejecutar√° autom√°ticamente con `confirm=True`.**

Cuando recibas la salida de una herramienta, anal√≠zala, res√∫mela y pres√©ntala al usuario de forma clara y amigable, utilizando formato Markdown si es apropiado.

El usuario te est√° dando permiso para que operes en su sistema. Act√∫a de forma proactiva para completar sus peticiones.
""")

from kogniterm.core.exceptions import UserConfirmationRequired # Importaci√≥n correcta

# --- Nodos del Grafo ---

from rich.live import Live # Importar Live
from rich.markdown import Markdown # Importar Markdown
from rich.padding import Padding # Nueva importaci√≥n
from rich.status import Status # ¬°Nueva importaci√≥n!
def handle_tool_confirmation(state: AgentState, llm_service: LLMService):
    """
    Maneja la respuesta de confirmaci√≥n del usuario para una operaci√≥n de herramienta.
    Si se aprueba, re-ejecuta la herramienta.
    """
    logger.debug(f"handle_tool_confirmation - Inicio. state.messages: {state.messages}")
    last_message = state.messages[-1]
    if not isinstance(last_message, ToolMessage):
        # Esto no deber√≠a pasar si el flujo es correcto
        console.print("[bold red]Error: handle_tool_confirmation llamado sin un ToolMessage.[/bold red]")
        state.reset_tool_confirmation()
        return state

    tool_message_content = last_message.content
    tool_id = state.tool_call_id_to_confirm # Usar el tool_id guardado

    # Asumimos que el ToolMessage de confirmaci√≥n tiene un formato espec√≠fico
    # ej. "Confirmaci√≥n de usuario: Aprobado para 'escribir en el archivo ...'".
    if "Aprobado" in tool_message_content:
        console.print("[bold green]‚úÖ Confirmaci√≥n de usuario recibida: Aprobado.[/bold green]")
        tool_name = state.tool_pending_confirmation
        tool_args = state.tool_args_pending_confirmation

        if tool_name and tool_args:
            console.print(f"[bold blue]üõ†Ô∏è Re-ejecutando herramienta '{tool_name}' tras aprobaci√≥n:[/bold blue]")

            tool = llm_service.get_tool(tool_name)
            if tool:
                # Si es file_update_tool o advanced_file_editor_tool, a√±adir el par√°metro confirm=True
                if tool_name == "file_update_tool" or tool_name == "advanced_file_editor":
                    tool_args["confirm"] = True
                    # Si el contenido original se pas√≥ como parte de tool_args,
                    # debemos asegurarnos de que el 'content' que se pasa para la re-ejecuci√≥n
                    # sea el contenido final que el usuario aprob√≥ (que deber√≠a estar en tool_args).
                    # No necesitamos el diff aqu√≠, solo el contenido final.
                    # El diff ya se mostr√≥ al usuario para la confirmaci√≥n.
                    # Si el content es None, significa que el LLM no lo proporcion√≥, lo cual es un error.
                    if tool_args.get("content") is None:
                        error_output = "Error: El contenido a actualizar no puede ser None."
                        state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                        console.print(f"[bold red]‚ùå {error_output}[/bold red]")
                        state.reset_tool_confirmation()
                        return state

                try:
                    raw_tool_output = llm_service._invoke_tool_with_interrupt(tool, tool_args)
                    tool_output_str = str(raw_tool_output)
                    tool_messages = [ToolMessage(content=tool_output_str, tool_call_id=tool_id)]
                    state.messages.extend(tool_messages)
                    console.print(f"[green]‚ú® Herramienta '{tool_name}' re-ejecutada con √©xito.[/green]")
                    logger.debug(f"Estado de state.messages despu√©s de re-ejecuci√≥n en handle_tool_confirmation: {state.messages}")
                    logger.debug(f"command_output_ready_for_processing en handle_tool_confirmation: {state.command_output_ready_for_processing}")

                    # Eliminar el ToolMessage original de confirmaci√≥n del historial
                    # Esto evita que el LLM lo procese nuevamente y genere respuestas redundantes.
                    if len(state.messages) >= 2 and isinstance(state.messages[-2], ToolMessage) and state.messages[-2].tool_call_id == tool_id:
                        state.messages.pop(-2) # Eliminar el ToolMessage anterior que solicitaba confirmaci√≥n
                    # No a√±adir un AIMessage de √©xito aqu√≠; el ToolMessage con la salida real es suficiente.
                    # El LLM debe procesar el ToolMessage directamente y continuar.
                except InterruptedError:
                    console.print("[bold yellow]‚ö†Ô∏è Re-ejecuci√≥n de herramienta interrumpida por el usuario. Volviendo al input.[/bold yellow]")
                    state.reset_temporary_state() # Limpiar el estado temporal del agente
                    return state # Terminar la ejecuci√≥n de herramientas y volver al input del usuario
                except Exception as e:
                    error_output = f"Error al re-ejecutar la herramienta {tool_name} tras aprobaci√≥n: {e}"
                    state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                    console.print(f"[bold red]‚ùå {error_output}[/bold red]")
            else:
                error_output = f"Error: Herramienta '{tool_name}' no encontrada para re-ejecuci√≥n."
                state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                console.print(f"[bold red]‚ùå {error_output}[/bold red]")
        else:
            error_output = "Error: No se encontr√≥ informaci√≥n de la herramienta pendiente para re-ejecuci√≥n."
            state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
            console.print(f"[bold red]‚ùå {error_output}[/bold red]")
    else:
        console.print("[bold yellow]‚ö†Ô∏è Confirmaci√≥n de usuario recibida: Denegado.[/bold yellow]")
        tool_output_str = f"Operaci√≥n denegada por el usuario: {state.tool_pending_confirmation or state.tool_code_tool_name}"
        state.messages.append(ToolMessage(content=tool_output_str, tool_call_id=tool_id))

    state.reset_tool_confirmation() # Limpiar el estado de confirmaci√≥n
    state.tool_call_id_to_confirm = None # Limpiar tambi√©n el tool_call_id guardado
    state.file_update_diff_pending_confirmation = None # Asegurarse de limpiar esto
    state.command_to_confirm = None # Asegurarse de limpiar esto tambi√©n
    logger.debug(f"handle_tool_confirmation - state.messages antes de retornar: {state.messages}")
    logger.debug(f"handle_tool_confirmation - command_output_ready_for_processing antes de retornar: {state.command_output_ready_for_processing}")
    return state

def call_model_node(state: AgentState, llm_service: LLMService, interrupt_queue: Optional[queue.Queue] = None):
    """Llama al LLM con el historial actual de mensajes y obtiene el resultado final, mostrando el streaming en Markdown."""
    last_message = state.messages[-1] if state.messages else None
    logger.debug(f"call_model_node - last_message TYPE: {type(last_message).__name__}")
    logger.debug(f"call_model_node - last_message CONTENT: {getattr(last_message, 'content', 'N/A')}")
    if isinstance(last_message, AIMessage) and getattr(last_message, 'tool_calls', None):
        logger.debug(f"call_model_node - last_message TOOL_CALLS: {last_message.tool_calls}")
    logger.debug(f"call_model_node - command_output_ready_for_processing: {state.command_output_ready_for_processing}")

    # El historial para la API es directamente el historial del estado del agente,
    # ya que AgentState.load_history() se encarga de asegurar el SYSTEM_MESSAGE.
    history_for_llm_call = state.messages

    # A√±adir DEBUG print para mostrar el historial completo antes de la llamada al LLM
    logger.debug(f"call_model_node - Historial completo enviado al LLM: {history_for_llm_call}")

    full_response_content = ""
    final_ai_message_from_llm = None
    text_streamed = False # Bandera para saber si hubo contenido de texto transmitido

    # Usar Live para actualizar el contenido en tiempo real
    with Live(console=console, screen=False, refresh_per_second=4) as live:
        for part in llm_service.invoke(history=history_for_llm_call, interrupt_queue=interrupt_queue):
            if isinstance(part, AIMessage):
                final_ai_message_from_llm = part
                # No acumulamos el contenido aqu√≠ si ya lo hemos hecho con los chunks de str
                # El full_response_content ya deber√≠a estar completo por los chunks de str
            elif isinstance(part, str): # Asegurarse de que 'part' es una cadena antes de concatenar
                # Este 'part' es un chunk de texto (str)
                full_response_content += part
                text_streamed = True # Hubo streaming de texto
                # Actualizar el contenido de Live con el Markdown acumulado
                live.update(Padding(Markdown(full_response_content), (1, 4)))

    # --- L√≥gica del Agente despu√©s de recibir la respuesta completa del LLM ---

    # Si hubo tool_calls, el AIMessage ya los contendr√°.

    # Si hubo tool_calls, el AIMessage ya los contendr√°.
    # No necesitamos imprimir el ToolMessage aqu√≠, ya que su contenido ya fue impreso en command_approval_handler.py

    # Si hubo tool_calls, el AIMessage ya los contendr√°.
    # No necesitamos imprimir el ToolMessage aqu√≠, ya que su contenido ya fue impreso en command_approval_handler.py

    if final_ai_message_from_llm and final_ai_message_from_llm.tool_calls:
        # El AIMessage final para el historial debe contener el contenido completo
        # y los tool_calls.
        ai_message_for_history = AIMessage(content=full_response_content, tool_calls=final_ai_message_from_llm.tool_calls)
        
        state.messages.append(ai_message_for_history)
        
        # Si la herramienta es 'execute_command', establecemos command_to_confirm
        command_to_execute = None
        tool_call_id = None # Inicializar tool_call_id
        if final_ai_message_from_llm.tool_calls:
            # Siempre capturar el tool_call_id del primer tool_call si existe
            tool_call_id = final_ai_message_from_llm.tool_calls[0]['id']

            for tc in final_ai_message_from_llm.tool_calls:
                if tc['name'] == 'execute_command':
                    command_to_execute = tc['args'].get('command')
                    break # Asumimos una sola llamada a comando por ahora

        return {
            "messages": state.messages,
            "command_to_confirm": command_to_execute, # Devolver el comando para confirmaci√≥n
            "tool_call_id_to_confirm": tool_call_id # Devolver el tool_call_id asociado
        }
    
    elif final_ai_message_from_llm: # Si es solo un AIMessage de texto (sin tool_calls)
        # El AIMessage final para el historial debe contener el contenido completo.
        ai_message_for_history = AIMessage(content=full_response_content)
        
        state.messages.append(ai_message_for_history)
        return {"messages": state.messages}
    else:
        # Fallback si por alguna raz√≥n no se obtuvo un AIMessage (poco probable con llm_service.py)
        error_message = "El modelo no proporcion√≥ una respuesta AIMessage v√°lida despu√©s de procesar los chunks."
        state.messages.append(AIMessage(content=error_message))
        return {"messages": state.messages}

def execute_tool_node(state: AgentState, llm_service: LLMService, terminal_ui: TerminalUI, interrupt_queue: Optional[queue.Queue] = None):
    """Ejecuta las herramientas solicitadas por el modelo."""
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:
        return state

    tool_messages = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call['name']
        tool_args = tool_call['args']
        tool_id = tool_call['id']

        # Verificar si hay una se√±al de interrupci√≥n
        if interrupt_queue and not interrupt_queue.empty():
            interrupt_queue.get() # Consumir la se√±al de interrupci√≥n
            console.print("[bold yellow]‚ö†Ô∏è Interrupci√≥n detectada. Volviendo al input del usuario.[/bold yellow]")
            state.reset_temporary_state() # Limpiar el estado temporal del agente
            return state # Terminar la ejecuci√≥n de herramientas y volver al input del usuario

        console.print(f"\n[bold blue]üõ†Ô∏è Ejecutando herramienta:[/bold blue] [yellow]{tool_name}[/yellow]")
        
        tool = llm_service.get_tool(tool_name)
        if not tool:
            tool_output_str = f"Error: Herramienta '{tool_name}' no encontrada."
            tool_messages.append(ToolMessage(content=tool_output_str, tool_call_id=tool_id))
            continue # Continuar con la siguiente herramienta si hay
        else:
            full_tool_output = "" # Acumular la salida completa de la herramienta
            try:
                # Iterar sobre el generador de la herramienta
                tool_output_generator = llm_service._invoke_tool_with_interrupt(tool, tool_args)
                
                # Si es un comando de ejecuci√≥n, la salida ya se maneja en command_approval_handler
                if tool_name == "execute_command":
                    # Solo acumular la salida, no imprimirla aqu√≠
                    for chunk in tool_output_generator:
                        full_tool_output += str(chunk)
                else:
                    # Para otras herramientas, acumular la salida sin imprimir en tiempo real
                    for chunk in tool_output_generator:
                        full_tool_output += str(chunk)
                
                # Despu√©s de que el generador termine, procesar la salida acumulada
                # La salida final para el LLM es `full_tool_output`.
                
            except UserConfirmationRequired as e:
                # Si la herramienta requiere confirmaci√≥n, guardamos el estado y terminamos la ejecuci√≥n de herramientas.
                state.tool_pending_confirmation = e.tool_name
                state.tool_args_pending_confirmation = e.tool_args
                state.tool_call_id_to_confirm = tool_id # Guardar el tool_id original
                state.file_update_diff_pending_confirmation = e.raw_tool_output # Guardar el diccionario completo
                
                console.print(f"[bold yellow]‚ö†Ô∏è Herramienta '{e.tool_name}' requiere confirmaci√≥n:[/bold yellow] {e.message}")
                # A√±adir un ToolMessage al estado para que el command_approval_handler lo procese
                # Asegurarse de que el diff se muestre como bloque de c√≥digo Markdown
                diff_content = e.raw_tool_output.get("diff", "")
                if isinstance(diff_content, str):
                    tool_messages.append(ToolMessage(content=json.dumps({"status": "requires_confirmation", "diff": f"```diff\n{diff_content}\n```"}), tool_call_id=tool_id))
                else:
                    tool_messages.append(ToolMessage(content=json.dumps(e.raw_tool_output), tool_call_id=tool_id))
                return state # Terminar la ejecuci√≥n de herramientas y permitir que should_continue decida
            except InterruptedError:
                console.print("[bold yellow]‚ö†Ô∏è Ejecuci√≥n de herramienta interrumpida por el usuario. Volviendo al input.[/bold yellow]")
                state.reset_temporary_state() # Limpiar el estado temporal del agente
                return state # Terminar la ejecuci√≥n de herramientas y volver al input del usuario
            except Exception as e:
                tool_output_str = f"Error al ejecutar la herramienta {tool_name}: {e}"
                tool_messages.append(ToolMessage(content=tool_output_str, tool_call_id=tool_id))
                continue # Continuar con la siguiente herramienta si hay

            # --- Procesar y mostrar el mensaje descriptivo de la herramienta ---
            # Para execute_command, el ToolMessage debe contener la salida completa del comando.
            # La l√≥gica de confirmaci√≥n se maneja por separado.
            if tool_name == "execute_command":
                command_to_execute = tool_args['command']
                state.command_to_confirm = command_to_execute
                state.tool_call_id_to_confirm = tool_id
                state.command_explanation = f"`{command_to_execute}`"

                # A√±adir un ToolMessage expl√≠cito para la confirmaci√≥n pendiente
                tool_messages.append(ToolMessage(
                    content=json.dumps({
                        "status": "requires_confirmation",
                        "operation": tool_name,
                        "command": command_to_execute,
                        "action_description": state.command_explanation
                    }),
                    tool_call_id=tool_id
                ))
                # No a√±adir la salida completa del comando aqu√≠, ya que est√° pendiente de confirmaci√≥n
                # y el ToolMessage de confirmaci√≥n ya se a√±adi√≥.
                # El grafo terminar√° aqu√≠ y esperar√° la interacci√≥n del usuario.
            else:
                # L√≥gica para herramientas que requieren confirmaci√≥n (file_update_tool, advanced_file_editor)
                try:
                    json_output = json.loads(full_tool_output)
                    should_confirm = False
                    confirmation_data = None

                    if isinstance(json_output, list) and all(isinstance(item, dict) for item in json_output):
                        for item in json_output:
                            if item.get("status") == "requires_confirmation" and (tool_name == "file_update_tool" or tool_name == "advanced_file_editor"):
                                should_confirm = True
                                confirmation_data = item
                                break
                    elif isinstance(json_output, dict):
                        if json_output.get("status") == "requires_confirmation" and (tool_name == "file_update_tool" or tool_name == "advanced_file_editor"):
                            should_confirm = True
                            confirmation_data = json_output

                    if should_confirm and confirmation_data:
                        state.file_update_diff_pending_confirmation = confirmation_data # Guardar el diccionario completo
                        state.tool_pending_confirmation = tool_name # Guardar el nombre de la herramienta
                        state.tool_args_pending_confirmation = tool_args # Guardar los argumentos originales
                        state.tool_call_id_to_confirm = tool_id # Guardar el tool_id original

                        # El ToolMessage ya se a√±adi√≥ con la salida real. Ahora, el grafo terminar√°
                        # y KogniTermApp manejar√° la confirmaci√≥n bas√°ndose en el estado.
                        return state # Terminar la ejecuci√≥n de herramientas y volver al input del usuario
                except json.JSONDecodeError:
                    pass # No es un JSON, continuar con el flujo normal

                # Para todas las herramientas (excepto execute_command que ya se manej√≥ arriba), a√±adir el ToolMessage con la salida completa
                processed_tool_output = full_tool_output # Ya no truncamos aqu√≠
                logger.debug(f"Longitud de full_tool_output en execute_tool_node: {len(full_tool_output)}")
                tool_messages.append(ToolMessage(content=processed_tool_output, tool_call_id=tool_id))

    state.messages.extend(tool_messages) # A√±adir todos los ToolMessages acumulados
    return state

# --- L√≥gica Condicional del Grafo ---

def should_continue(state: AgentState) -> str:
    last_message = state.messages[-1]
    logger.debug(f"should_continue - command_to_confirm: {state.command_to_confirm}, file_update_diff_pending_confirmation: {state.file_update_diff_pending_confirmation}")
    logger.debug(f"should_continue - last_message TYPE: {type(last_message).__name__}")
    logger.debug(f"should_continue - last_message CONTENT: {getattr(last_message, 'content', 'N/A')}")

    # Si hay un comando pendiente de confirmaci√≥n, siempre terminamos el grafo aqu√≠
    # para que la terminal lo maneje.
    if state.command_to_confirm or state.file_update_diff_pending_confirmation:
        logger.debug("should_continue - Retornando END por confirmaci√≥n pendiente.")
        return END

    # Si el √∫ltimo mensaje del AI tiene tool_calls, ejecutar herramientas
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        logger.debug("should_continue - Retornando execute_tool.")
        return "execute_tool"
    # Si el √∫ltimo mensaje es un ToolMessage (resultado de una herramienta)
    # y no hay confirmaci√≥n pendiente, entonces la tarea ha finalizado o
    # se espera una nueva entrada del usuario.
    elif isinstance(last_message, ToolMessage):
        # Si el √∫ltimo mensaje es un ToolMessage (resultado de una herramienta),
        # siempre debemos volver a call_model para que el LLM procese la salida.
        logger.debug("should_continue - Retornando call_model (despu√©s de ToolMessage).")
        return "call_model"
    elif isinstance(last_message, HumanMessage): # Si el √∫ltimo mensaje es un HumanMessage, siempre llamar al modelo
        logger.debug("should_continue - Retornando call_model (despu√©s de HumanMessage).")
        return "call_model"
    elif isinstance(last_message, AIMessage) and not last_message.tool_calls:
        # Si el √∫ltimo mensaje es un AIMessage sin tool_calls, terminar el grafo
        logger.debug("should_continue - Retornando END (AIMessage sin tool_calls).")
        return END
    else:
        logger.debug("should_continue - Retornando END (condici√≥n por defecto).")
        return END

# --- Construcci√≥n del Grafo ---

def create_bash_agent(llm_service: LLMService, terminal_ui: TerminalUI, interrupt_queue: Optional[queue.Queue] = None):
    bash_agent_graph = StateGraph(AgentState)

    bash_agent_graph.add_node("call_model", functools.partial(call_model_node, llm_service=llm_service, interrupt_queue=interrupt_queue))
    bash_agent_graph.add_node("execute_tool", functools.partial(execute_tool_node, llm_service=llm_service, terminal_ui=terminal_ui, interrupt_queue=interrupt_queue))

    bash_agent_graph.set_entry_point("call_model")

    bash_agent_graph.add_conditional_edges(
        "call_model",
        should_continue,
        {
            "execute_tool": "execute_tool",
            "call_model": "call_model", # A√±adir esta l√≠nea
            END: END
        }
    )

    bash_agent_graph.add_edge("execute_tool", "call_model")

    return bash_agent_graph.compile()


