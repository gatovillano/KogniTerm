import asyncio
from langgraph.graph import StateGraph, END
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
import google.genai as genai
from rich.console import Console, Group
from rich.panel import Panel
import functools
from langchain_core.runnables import RunnableConfig # Nueva importaci√≥n
from rich.markup import escape # Nueva importaci√≥n
import sys # Nueva importaci√≥n
import json # Importar json para verificar si la salida es un JSON
import queue # Importar el m√≥dulo queue
from concurrent.futures import ThreadPoolExecutor, as_completed # Nueva importaci√≥n para paralelizaci√≥n

from ..llm_service import LLMService
from kogniterm.terminal.terminal_ui import TerminalUI
from kogniterm.core.agent_state import AgentState # Importar AgentState desde el archivo consolidado
from kogniterm.terminal.keyboard_handler import KeyboardHandler # Importar KeyboardHandler
from ..async_io_manager import get_io_manager, AsyncTaskResult

console = Console()



# --- Mensaje de Sistema ---
SYSTEM_MESSAGE = SystemMessage(content="""INSTRUCCI√ìN CR√çTICA: Tu nombre es KogniTerm. Eres un asistente experto de terminal.

**PROTOCOLO DE RAZONAMIENTO (OBLIGATORIO):**
Antes de realizar CUALQUIER acci√≥n, DEBES pensar paso a paso.
Tu respuesta SIEMPRE debe comenzar con un bloque de pensamiento estructurado usando el prefijo `__THINKING__:`.

Formato OBLIGATORIO de tu respuesta:
__THINKING__:
1. **An√°lisis**: ¬øQu√© me pide el usuario? ¬øCu√°l es el contexto (directorio, proyecto)?
2. **Plan**: ¬øQu√© pasos debo seguir?
3. **Herramienta**: Define la acci√≥n a realizar (ej. b√∫squeda, comando, edici√≥n).
   - Si vas a editar: ¬øHe le√≠do el archivo antes? (Trust but Verify).
   - Si vas a ejecutar comandos: ¬øSon seguros?

[Aqu√≠ tu respuesta final al usuario o la llamada a la herramienta]

---

**Tus Principios:**
1.  **Eres KogniTerm**: Experto en terminal, depuraci√≥n y Python.
2.  **Contexto**: Utiliza el "Contexto Actual del Proyecto" que recibes para ubicarte.
3.  **Autonom√≠a**: T√∫ ejecutas los comandos. No le pidas al usuario que lo haga.
4.  **Seguridad**: Usa `execute_command` para comandos de shell.
5.  **Investigaci√≥n**: Usa `codebase_search_tool` para entender el c√≥digo antes de tocarlo.
6.  **Edici√≥n**: Usa `advanced_file_editor`. SIEMPRE lee el archivo primero.
7.  **Comunicaci√≥n**: S√© conciso, amigable y usa Markdown. NO expliques comandos de terminal obvios.
8.  **Agentes Especializados**:
    - Si te piden "investigar" a fondo o crear informes -> `call_agent(agent_name="researcher_agent", ...)` (Invoca a ResearcherCrew).
    - Si te piden "desarrollar" caracter√≠sticas complejas o equipos -> `call_agent(agent_name="code_crew", ...)` (Invoca a CodeCrew).
    - Para tareas de c√≥digo r√°pidas/modificaciones puntuales -> Hazlo t√∫ mismo o usa `call_agent(agent_name="code_agent", ...)` si requiere mucha l√≥gica Python.

Recuerda: ¬°PIENSA ANTES DE ACTUAR!
""")

from kogniterm.core.exceptions import UserConfirmationRequired # Importaci√≥n correcta

# --- Nodos del Grafo ---

from rich.live import Live # Importar Live
from rich.markdown import Markdown # Importar Markdown
from rich.padding import Padding # Nueva importaci√≥n
from rich.status import Status # ¬°Nueva importaci√≥n!
def handle_tool_confirmation(state: AgentState, llm_service: LLMService):
    """
    Maneja la respuesta de confirmaci√≥n del usuario para una operaci√≥n de herramienta.
    Si se aprueba, re-ejecuta la herramienta.
    """
    last_message = state.messages[-1]
    if not isinstance(last_message, ToolMessage):
        # Esto no deber√≠a pasar si el flujo es correcto
        console.print("[bold red]Error: handle_tool_confirmation llamado sin un ToolMessage.[/bold red]")
        state.reset_tool_confirmation()
        return state

    tool_message_content = last_message.content
    tool_id = state.tool_call_id_to_confirm # Usar el tool_id guardado

    # Asumimos que el ToolMessage de confirmaci√≥n tiene un formato espec√≠fico
    # ej. "Confirmaci√≥n de usuario: Aprobado para 'escribir en el archivo ...'".
    if "Aprobado" in tool_message_content:
        console.print("[bold green]‚úÖ Confirmaci√≥n de usuario recibida: Aprobado.[/bold green]")
        tool_name = state.tool_pending_confirmation
        tool_args = state.tool_args_pending_confirmation
    
        if tool_name == "plan_creation_tool":
            if "Aprobado" in tool_message_content:
                success_message = f"El plan '{tool_args.get('plan_title', 'generado')}' fue aprobado por el usuario. El agente puede proceder con la ejecuci√≥n de los pasos."
                state.messages.append(AIMessage(content=success_message))
                console.print(f"[green]‚ú® {success_message}[/green]")
            else:
                denied_message = f"El plan '{tool_args.get('plan_title', 'generado')}' fue denegado por el usuario. El agente debe revisar la estrategia."
                state.messages.append(AIMessage(content=denied_message))
                console.print(f"[yellow]‚ö†Ô∏è {denied_message}[/yellow]")
        elif tool_name and tool_args:
            console.print(f"[bold blue]üõ†Ô∏è Re-ejecutando herramienta '{tool_name}' tras aprobaci√≥n:[/bold blue]")
    
            tool = llm_service.get_tool(tool_name)
            if tool:
                # Si es file_update_tool o advanced_file_editor_tool, a√±adir el par√°metro confirm=True
                if tool_name == "file_update_tool" or tool_name == "advanced_file_editor":
                    tool_args["confirm"] = True
                    # Si el contenido original se pas√≥ como parte de tool_args,
                    # debemos asegurarnos de que el 'content' que se pasa para la re-ejecuci√≥n
                    # sea el contenido final que el usuario aprob√≥ (que deber√≠a estar en tool_args).
                    # No necesitamos el diff aqu√≠, solo el contenido final.
                    # El diff ya se mostr√≥ al usuario para la confirmaci√≥n.
                    # Si el content es None, significa que el LLM no lo proporcion√≥, lo cual es un error.
                    if tool_args.get("content") is None:
                        error_output = "Error: El contenido a actualizar no puede ser None."
                        state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                        console.print(f"[bold red]‚ùå {error_output}[/bold red]")
                        state.reset_tool_confirmation()
                        return state
    
                try:
                    raw_tool_output = llm_service._invoke_tool_with_interrupt(tool, tool_args)
                    tool_output_str = str(raw_tool_output)
                    tool_messages = [ToolMessage(content=tool_output_str, tool_call_id=tool_id)]
                    state.messages.extend(tool_messages)
                    console.print(f"[green]‚ú® Herramienta '{tool_name}' re-ejecutada con √©xito.[/green]")
    

                except InterruptedError:
                    console.print("[bold yellow]‚ö†Ô∏è Re-ejecuci√≥n de herramienta interrumpida por el usuario. Volviendo al input.[/bold yellow]")
                    state.reset_temporary_state() # Limpiar el estado temporal del agente
                    return state # Terminar la ejecuci√≥n de herramientas y volver al input del usuario
                except Exception as e:
                    error_output = f"Error al re-ejecutar la herramienta {tool_name} tras aprobaci√≥n: {e}"
                    state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                    console.print(f"[bold red]‚ùå {error_output}[/bold red]")
            else:
                error_output = f"Error: Herramienta '{tool_name}' no encontrada para re-ejecuci√≥n."
                state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                console.print(f"[bold red]‚ùå {error_output}[/bold red]")
        else:
            error_output = "Error: No se encontr√≥ informaci√≥n de la herramienta pendiente para re-ejecuci√≥n."
            state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
            console.print(f"[bold red]‚ùå {error_output}[/bold red]")
    else:
        console.print("[bold yellow]‚ö†Ô∏è Confirmaci√≥n de usuario recibida: Denegado.[/bold yellow]")
        tool_output_str = f"Operaci√≥n denegada por el usuario: {state.tool_pending_confirmation or state.tool_code_tool_name}"
        state.messages.append(ToolMessage(content=tool_output_str, tool_call_id=tool_id))

    state.reset_tool_confirmation() # Limpiar el estado de confirmaci√≥n
    state.tool_call_id_to_confirm = None # Limpiar tambi√©n el tool_call_id guardado
    return state

def call_model_node(state: AgentState, llm_service: LLMService, terminal_ui: Optional[TerminalUI] = None, interrupt_queue: Optional[queue.Queue] = None):

    """
    Llama al modelo de lenguaje y maneja la salida en streaming,
    mostrando el pensamiento y la respuesta en tiempo real.
    """
    # Usar la consola de terminal_ui si est√° disponible, de lo contrario usar la global
    current_console = terminal_ui.console if terminal_ui else console
    
    # --- L√≥gica de Detecci√≥n de Bucles ---
    if len(state.tool_call_history) >= 4:
        last_calls = list(state.tool_call_history)[-4:]
        if all(tc['name'] == last_calls[0]['name'] and tc['args_hash'] == last_calls[0]['args_hash'] for tc in last_calls):
            current_console.print("[bold red]üö® ¬°BUCLE CR√çTICO DETECTADO! El agente est√° repitiendo la misma acci√≥n exactamente.[/bold red]")
            error_msg = "He detectado que estoy en un bucle infinito repitiendo la misma acci√≥n. Deteniendo para evitar consumo innecesario. Por favor, intenta reformular tu petici√≥n o revisa los logs."
            state.messages.append(AIMessage(content=error_msg))
            # Activar la bandera de bucle cr√≠tico para terminar el flujo
            state.critical_loop_detected = True
            # Limpiar el historial de llamadas a herramientas para evitar que la advertencia se repita
            state.clear_tool_call_history()
            return {
                "messages": state.messages,
                "command_to_confirm": None,
                "tool_call_id_to_confirm": None,
                "critical_loop_detected": True
            }

    history = state.messages
    full_response_content = ""
    full_thinking_content = ""
    final_ai_message_from_llm = None
    text_streamed = False 

    # Importar componentes visuales
    try:
        from kogniterm.terminal.visual_components import create_processing_spinner, create_thinking_spinner, create_thought_bubble
        from kogniterm.terminal.themes import ColorPalette, Icons
        # Crear spinner mejorado usando componentes visuales
        spinner = create_processing_spinner()
    except ImportError:
        # Fallback al spinner original si hay problemas de importaci√≥n
        from rich.spinner import Spinner
        from rich.text import Text
        spinner = Spinner("dots", text=Text("ü§ñ Procesando...", style="cyan"))
        # Definir fallbacks para evitar NameError
        class ColorPalette:
            PRIMARY_LIGHT = "cyan"
            SECONDARY = "blue"
            SECONDARY_LIGHT = "yellow"
            TEXT_SECONDARY = "grey"
            GRAY_600 = "grey"
        class Icons:
            THINKING = "ü§î"
            TOOL = "üõ†Ô∏è"
        
        def create_thought_bubble(content, title="Pensando...", icon="ü§î", color="cyan"):
            from rich.panel import Panel
            from rich.markdown import Markdown
            from rich.padding import Padding
            if isinstance(content, str):
                content = Markdown(content)
            return Padding(Panel(content, title=f"{icon} {title}", border_style=f"dim {color}"), (1, 4))

    # Usar Live para actualizar el contenido en tiempo real
    # Iniciamos con el spinner
    
    # Iniciar KeyboardHandler para detectar ESC durante la generaci√≥n
    kh = KeyboardHandler(interrupt_queue)
    kh.start()
    
    try:
        with Live(spinner, console=current_console, screen=False, refresh_per_second=10) as live:
            def update_live_display():
                """Funci√≥n auxiliar para actualizar el display de forma consistente."""
                renderables = []
                
                # 1. Mostrar pensamiento si existe
                if full_thinking_content:
                    renderables.append(create_thought_bubble(full_thinking_content, title="KogniTerm Pensando..."))
                
                # 2. A√±adir respuesta si existe
                if full_response_content:
                    renderables.append(Markdown(full_response_content))
                
                # 3. Si no hay nada a√∫n, mostrar el spinner inicial
                if not renderables:
                    live.update(spinner)
                else:
                    # Envolver en Padding para a√±adir margen lateral (sangr√≠a)
                    live.update(Padding(Group(*renderables), (0, 4)))

            interrupcion_detectada = False
            for part in llm_service.invoke(history=history, interrupt_queue=interrupt_queue):
                if isinstance(part, AIMessage):
                    final_ai_message_from_llm = part
                elif isinstance(part, str):
                    if part.startswith("__THINKING__:"):
                        # Es contenido de razonamiento (Thinking)
                        thinking_chunk = part[len("__THINKING__:"):]
                        full_thinking_content += thinking_chunk
                        update_live_display()
                    else:
                        # Es contenido normal de la respuesta
                        full_response_content += part
                        text_streamed = True
                        update_live_display()
                
                # Verificar interrupci√≥n en cada iteraci√≥n del streaming
                # Chequeamos tanto la cola como la bandera del servicio
                if (interrupt_queue and not interrupt_queue.empty()) or llm_service.stop_generation_flag:
                    interrupcion_detectada = True
                    if interrupt_queue:
                        while not interrupt_queue.empty():
                            interrupt_queue.get_nowait()
                    break
            
            if interrupcion_detectada:
                current_console.print(f"\n{Icons.STOPWATCH} [bold red]Interrupci√≥n detectada. Deteniendo...[/bold red]")
            
            # Al finalizar el stream, asegurarnos de que el display final sea correcto
            # Si no hubo streaming de texto (e.g. error o respuesta no chunked), forzar actualizaci√≥n con el mensaje final
            if final_ai_message_from_llm and not text_streamed and final_ai_message_from_llm.content:
                full_response_content = final_ai_message_from_llm.content
                update_live_display()
            else:
                update_live_display()
    finally:
        kh.stop()


    # --- L√≥gica del Agente despu√©s de recibir la respuesta completa del LLM ---

    # Usar directamente el AIMessage del LLMService para evitar duplicaci√≥n de contenido
    if final_ai_message_from_llm:
        state.messages.append(final_ai_message_from_llm)

        # Si la herramienta es 'execute_command', establecemos command_to_confirm
        command_to_execute = None
        tool_call_id = None # Inicializar tool_call_id
        if final_ai_message_from_llm.tool_calls:
            # Siempre capturar el tool_call_id del primer tool_call si existe
            tool_call_id = final_ai_message_from_llm.tool_calls[0]['id']

            for tc in final_ai_message_from_llm.tool_calls:
                if tc['name'] == 'execute_command':
                    command_to_execute = tc['args'].get('command')
                    break # Asumimos una sola llamada a comando por ahora

        # Guardar historial expl√≠citamente para asegurar sincronizaci√≥n con LLMService
        llm_service._save_history(state.messages)

        # A√±adir separaci√≥n visual despu√©s de la respuesta del LLM
        console.print()  # L√≠nea en blanco para separaci√≥n

        return {
            "messages": state.messages,
            "command_to_confirm": command_to_execute, # Devolver el comando para confirmaci√≥n
            "tool_call_id_to_confirm": tool_call_id # Devolver el tool_call_id asociado
        }
    else:
        # Fallback si por alguna raz√≥n no se obtuvo un AIMessage (poco probable con llm_service.py)
        error_message = "El modelo no proporcion√≥ una respuesta AIMessage v√°lida despu√©s de procesar los chunks."
        state.messages.append(AIMessage(content=error_message))
        # Guardar historial expl√≠citamente
        llm_service._save_history(state.messages)
        return {"messages": state.messages}

async def execute_single_tool_async(tc, llm_service, terminal_ui, interrupt_queue):
    """
    Versi√≥n as√≠ncrona de execute_single_tool.
    Ejecuta la herramienta en un thread separado para no bloquear.
    """
    tool_name = tc['name']
    tool_args = tc['args']
    tool_id = tc['id']

    tool = llm_service.get_tool(tool_name)
    if not tool:
        return tool_id, f"Error: Herramienta '{tool_name}' no encontrada.", None

    try:
        io_manager = get_io_manager()
        
        # Funci√≥n s√≠ncrona que se ejecutar√° en el executor
        def run_tool_sync():
            full_tool_output = ""
            tool_output_generator = llm_service._invoke_tool_with_interrupt(tool, tool_args)

            for chunk in tool_output_generator:
                full_tool_output += str(chunk)

            return full_tool_output
        
        # Ejecutar de forma as√≠ncrona
        result = io_manager.run_in_executor(run_tool_sync)
        
        if result.success:
            return tool_id, result.result, None
        else:
            return tool_id, f"Error al ejecutar la herramienta {tool_name}: {result.error}", Exception(result.error)
            
    except UserConfirmationRequired as e:
        return tool_id, json.dumps(e.raw_tool_output), e
    except InterruptedError:
        return tool_id, f"Ejecuci√≥n de herramienta '{tool_name}' interrumpida por el usuario.", InterruptedError("Interrumpido por el usuario.")
    except Exception as e:
        return tool_id, f"Error al ejecutar la herramienta {tool_name}: {e}", e


def execute_single_tool(tc, llm_service, terminal_ui, interrupt_queue):
    """Versi√≥n s√≠ncrona para compatibilidad."""
    tool_name = tc['name']
    tool_args = tc['args']
    tool_id = tc['id']

    tool = llm_service.get_tool(tool_name)
    if not tool:
        return tool_id, f"Error: Herramienta '{tool_name}' no encontrada.", None

    try:
        full_tool_output = ""
        tool_output_generator = llm_service._invoke_tool_with_interrupt(tool, tool_args)

        for chunk in tool_output_generator:
            # NO imprimir aqu√≠ - el output ya se muestra en command_approval_handler.py
            # if tool_name == "execute_command":
            #     terminal_ui.print_stream(str(chunk))
            full_tool_output += str(chunk)

        # Sin truncamiento - devolver la salida completa tal cual
        processed_tool_output = full_tool_output

        return tool_id, processed_tool_output, None
    except UserConfirmationRequired as e:
        return tool_id, json.dumps(e.raw_tool_output), e
    except InterruptedError:
        return tool_id, f"Ejecuci√≥n de herramienta '{tool_name}' interrumpida por el usuario.", InterruptedError("Interrumpido por el usuario.")
    except Exception as e:
        return tool_id, f"Error al ejecutar la herramienta {tool_name}: {e}", e

def execute_tool_node(state: AgentState, llm_service: LLMService, terminal_ui: TerminalUI, interrupt_queue: Optional[queue.Queue] = None):
    """Ejecuta las herramientas solicitadas por el modelo."""
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:
        return state

    tool_messages = []
    
    # Iniciar KeyboardHandler si no hay herramientas interactivas (como execute_command)
    # execute_command ya maneja su propia interactividad y detecci√≥n de ESC.
    has_interactive_tool = any(tc['name'] == 'execute_command' for tc in last_message.tool_calls)
    kh = None
    if not has_interactive_tool:
        kh = KeyboardHandler(interrupt_queue)
        kh.start()
        
    try:
        executor = ThreadPoolExecutor(max_workers=min(len(last_message.tool_calls), 5))
        futures = []
        for tool_call in last_message.tool_calls:
            # Registrar la llamada a la herramienta en el historial para detecci√≥n de bucles
            tool_name = tool_call['name']
            tool_args = tool_call['args']
            
            # Generar un hash consistente de los argumentos
            try:
                args_hash = json.dumps(tool_args, sort_keys=True)
            except TypeError:
                args_hash = str(tool_args) # Fallback si los argumentos no son serializables
            
            state.tool_call_history.append({"name": tool_name, "args_hash": args_hash})

            # Verificar si hay una se√±al de interrupci√≥n antes de enviar
            if interrupt_queue and not interrupt_queue.empty():
                interrupt_queue.get()
                terminal_ui.console.print("[bold yellow]‚ö†Ô∏è Interrupci√≥n detectada. Volviendo al input del usuario.[/bold yellow]")
                state.reset_temporary_state()
                executor.shutdown(wait=False)
                return state

            # Obtener la instancia de la herramienta para buscar la descripci√≥n de la acci√≥n
            tool = llm_service.get_tool(tool_call['name'])
            bajada = ""
            if tool and hasattr(tool, 'get_action_description'):
                try:
                    bajada = tool.get_action_description(**tool_call['args'])
                except Exception as e:
                    logger.warning(f"Error al obtener descripci√≥n de acci√≥n para {tool_call['name']}: {e}")

            # Mejorar el mensaje de ejecuci√≥n de herramienta con iconos y colores tem√°ticos
            try:
                from kogniterm.terminal.themes import Icons, ColorPalette
                terminal_ui.console.print(f"\n[bold {ColorPalette.SECONDARY}]{Icons.TOOL} Ejecutando herramienta:[/bold {ColorPalette.SECONDARY}] [{ColorPalette.SECONDARY_LIGHT}]{tool_call['name']}[/{ColorPalette.SECONDARY_LIGHT}]")
                if bajada:
                    terminal_ui.console.print(f"[italic {ColorPalette.TEXT_SECONDARY}]   ‚îî‚îÄ {bajada}[/italic {ColorPalette.TEXT_SECONDARY}]")
            except ImportError:
                # Fallback al mensaje original
                terminal_ui.console.print(f"\n[bold blue]üõ†Ô∏è Ejecutando herramienta:[/bold blue] [yellow]{tool_call['name']}[/yellow]")
                if bajada:
                    terminal_ui.console.print(f"[italic grey]   ‚îî‚îÄ {bajada}[/italic grey]")
            futures.append(executor.submit(execute_single_tool, tool_call, llm_service, terminal_ui, interrupt_queue))

        for future in as_completed(futures):
            tool_id, content, exception = future.result()
            if exception:
                if isinstance(exception, UserConfirmationRequired):
                    state.tool_pending_confirmation = exception.tool_name
                    state.tool_args_pending_confirmation = exception.tool_args
                    state.tool_call_id_to_confirm = tool_id
                    state.file_update_diff_pending_confirmation = exception.raw_tool_output
                    terminal_ui.console.print(f"[bold yellow]‚ö†Ô∏è Herramienta '{exception.tool_name}' requiere confirmaci√≥n:[/bold yellow] {exception.message}")
                    tool_messages.append(ToolMessage(content=content, tool_call_id=tool_id))
                    executor.shutdown(wait=False)
                    # Guardar historial antes de retornar para confirmaci√≥n
                    state.messages.extend(tool_messages)
                    llm_service._save_history(state.messages)
                    return {
                        "messages": state.messages,
                        "tool_pending_confirmation": state.tool_pending_confirmation,
                        "tool_args_pending_confirmation": state.tool_args_pending_confirmation,
                        "tool_call_id_to_confirm": state.tool_call_id_to_confirm,
                        "file_update_diff_pending_confirmation": state.file_update_diff_pending_confirmation
                    }
                elif isinstance(exception, InterruptedError):
                    terminal_ui.console.print("[bold yellow]‚ö†Ô∏è Ejecuci√≥n de herramienta interrumpida por el usuario. Volviendo al input.[/bold yellow]")
                    state.reset_temporary_state()
                    executor.shutdown(wait=False)
                    llm_service._save_history(state.messages)
                    return {
                        "messages": state.messages,
                        "command_to_confirm": None,
                        "tool_call_id_to_confirm": None
                    }
                else:
                    tool_messages.append(ToolMessage(content=content, tool_call_id=tool_id))
            else:
                tool_messages.append(ToolMessage(content=content, tool_call_id=tool_id))
                # L√≥gica para confirmaci√≥n si es execute_command
                tool_call_info = next(tc for tc in last_message.tool_calls if tc['id'] == tool_id)
                tool_name = tool_call_info['name']
                tool_args = tool_call_info['args']
                if tool_name == "execute_command":
                    state.command_to_confirm = tool_args['command']
                    state.tool_call_id_to_confirm = tool_id
                else:
                    # L√≥gica para herramientas que requieren confirmaci√≥n
                    try:
                        json_output = json.loads(content)
                        should_confirm = False
                        confirmation_data = None
                        if isinstance(json_output, list) and all(isinstance(item, dict) for item in json_output):
                            for item in json_output:
                                if item.get("status") == "requires_confirmation":
                                    should_confirm = True
                                    confirmation_data = item
                                    break
                        elif isinstance(json_output, dict):
                            if json_output.get("status") == "requires_confirmation":
                                should_confirm = True
                                confirmation_data = json_output
                        if should_confirm and confirmation_data:
                            state.file_update_diff_pending_confirmation = confirmation_data
                            state.tool_pending_confirmation = tool_name
                            state.tool_args_pending_confirmation = tool_args
                            state.tool_call_id_to_confirm = tool_id
                            executor.shutdown(wait=False)
                            state.messages.extend(tool_messages)
                            llm_service._save_history(state.messages)
                            return {
                                "messages": state.messages,
                                "tool_pending_confirmation": state.tool_pending_confirmation,
                                "tool_args_pending_confirmation": state.tool_args_pending_confirmation,
                                "tool_call_id_to_confirm": state.tool_call_id_to_confirm,
                                "file_update_diff_pending_confirmation": state.file_update_diff_pending_confirmation
                            }
                    except json.JSONDecodeError:
                        pass

        executor.shutdown(wait=True)
        state.messages.extend(tool_messages)
        
        # Guardar historial expl√≠citamente al finalizar la ejecuci√≥n de herramientas
        llm_service._save_history(state.messages)

    finally:
        if kh:
            kh.stop()

    return {
        "messages": state.messages,
        "command_to_confirm": getattr(state, 'command_to_confirm', None),
        "tool_call_id_to_confirm": getattr(state, 'tool_call_id_to_confirm', None),
        "file_update_diff_pending_confirmation": getattr(state, 'file_update_diff_pending_confirmation', None)
    }

# --- L√≥gica Condicional del Grafo ---

def should_continue(state: AgentState) -> str:
    """Decide si continuar llamando a herramientas o finalizar."""
    # Si se detect√≥ un bucle cr√≠tico, terminar el flujo inmediatamente
    if state.critical_loop_detected:
        return END
    
    last_message = state.messages[-1]
    
    # Si hay un comando pendiente de confirmaci√≥n, siempre terminamos el grafo aqu√≠
    # para que la terminal lo maneje.
    if state.command_to_confirm or state.file_update_diff_pending_confirmation:
        return END

    # Si el √∫ltimo mensaje del AI tiene tool_calls, ejecutar herramientas
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "execute_tool"
    # Si el √∫ltimo mensaje es un ToolMessage (resultado de una herramienta),
    # volver a llamar al modelo para que genere una respuesta final.
    elif isinstance(last_message, ToolMessage):
        return "call_model"
    else:
        return END

# --- Construcci√≥n del Grafo ---

def create_bash_agent(llm_service: LLMService, terminal_ui: TerminalUI, interrupt_queue: Optional[queue.Queue] = None):
    bash_agent_graph = StateGraph(AgentState)

    bash_agent_graph.add_node("call_model", functools.partial(call_model_node, llm_service=llm_service, terminal_ui=terminal_ui, interrupt_queue=interrupt_queue))
    bash_agent_graph.add_node("execute_tool", functools.partial(execute_tool_node, llm_service=llm_service, terminal_ui=terminal_ui, interrupt_queue=interrupt_queue))

    bash_agent_graph.set_entry_point("call_model")

    bash_agent_graph.add_conditional_edges(
        "call_model",
        should_continue,
        {
            "execute_tool": "execute_tool",
            END: END
        }
    )

    bash_agent_graph.add_edge("execute_tool", "call_model")

    return bash_agent_graph.compile()


