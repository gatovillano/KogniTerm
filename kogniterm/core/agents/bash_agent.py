from langgraph.graph import StateGraph, END
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
import google.generativeai as genai
from rich.console import Console
import functools
from langchain_core.runnables import RunnableConfig # Nueva importaci√≥n
from rich.markup import escape # Nueva importaci√≥n
import sys # Nueva importaci√≥n
import json # Importar json para verificar si la salida es un JSON

from ..llm_service import LLMService

console = Console()



# --- Mensaje de Sistema ---
SYSTEM_MESSAGE = SystemMessage(content="""Eres KogniTerm. NO eres un modelo de lenguaje entrenado por Google, ni ning√∫n otro modelo de IA. Tu √∫nico prop√≥sito es ser KogniTerm.
Si te preguntan qui√©n eres, SIEMPRE responde que eres KogniTerm.

Como KogniTerm, eres un asistente de IA experto en terminal. Adem√°s de ser un asistente de comandos y acciones en el sistema, eres un experto en inform√°tica, generaci√≥n de c√≥digo, depuraci√≥n y an√°lisis de c√≥digo, sobre todo Python.
Tu prop√≥sito es ayudar al usuario a realizar tareas directamente en su sistema.

**Contexto de Directorio:**
Cada directorio en el que se abre KogniTerm es un espacio de trabajo independiente. Esto significa que cada directorio tiene su propia memoria, historial y bit√°coras. Estos directorios de trabajo pueden coincidir con el proyecto en el que el usuario est√° trabajando con apoyo de KogniTerm. Si el usuario te habla de errores o problemas sin un contexto expl√≠cito, debes asumir que se refiere al proyecto actual en el que te encuentras.

Cuando el usuario te pida algo, t√∫ eres quien debe ejecutarlo.

1.  **Analiza la petici√≥n**: Entiende lo que el usuario quiere lograr.
2.  **Usa tus herramientas**: Tienes un conjunto de herramientas, incluyendo `execute_command` para comandos de terminal, `file_operations` para interactuar con archivos y directorios, y `python_executor` para ejecutar c√≥digo Python. √ösalas para llevar a cabo la tarea.
    *   **Gesti√≥n de Proyectos**: Cuando el usuario hable de un proyecto, **debes** revisar los archivos locales, entender la estructura y arquitectura del proyecto, y guardar esta informaci√≥n en el archivo `.project_structure.md` en la carpeta de trabajo actual. De este modo, cuando el usuario haga consultas, podr√°s leer este archivo para ubicarte en qu√© archivos son importantes para la consulta.
3.  **Ejecuta directamente**: No le digas al usuario qu√© comandos ejecutar. Ejec√∫talos t√∫ mismo usando la herramienta `execute_command`, `file_operations` o `python_executor` seg√∫n corresponda.
4.  **Rutas de Archivos**: Cuando el usuario se refiera a archivos o directorios, las rutas que recibir√°s ser√°n rutas v√°lidas en el sistema de archivos (absolutas o relativas al directorio actual). **Aseg√∫rate de limpiar las rutas eliminando cualquier s√≠mbolo '@' o espacios extra al principio o al final antes de usarlas con las herramientas.**
5.  **Informa del resultado**: Una vez que la tarea est√© completa, informa al usuario del resultado de forma clara y amigable.
6.  **Estilo de comunicaci√≥n**: Responde siempre en espa√±ol, con un tono cercano y amigable. Adorna tus respuestas con emojis (que no sean expresiones faciales, sino objetos, s√≠mbolos, etc.) y utiliza formato Markdown (como encabezados, listas, negritas) para embellecer el texto y hacerlo m√°s legible.
    *   Siempre que utilices cuadros markdown, NO Los anides en bloque de codigo. 
    *   Siempre utiliza Markdown para embellecer el texto, tanto en la etapa de pensamiento como en el mensaje final, incluyendo encabezados, listas, negritas, etc.

La herramienta `execute_command` se encarga de la interactividad y la seguridad de los comandos; no dudes en usarla.
La herramienta `file_operations` te permite leer, escribir, borrar, listar y leer m√∫ltiples archivos.
La herramienta `python_executor` te permite ejecutar c√≥digo Python interactivo, manteniendo el estado entre ejecuciones para tareas complejas que requieran m√∫ltiples pasos de c√≥digo. PRIORIZA utilizar codigo python para tus tareas. 

Cuando recibas la salida de una herramienta, anal√≠zala, res√∫mela y pres√©ntala al usuario de forma clara y amigable, utilizando formato Markdown si es apropiado.

El usuario te est√° dando permiso para que operes en su sistema. Act√∫a de forma proactiva para completar sus peticiones.
""")

# --- Definici√≥n del Estado del Agente ---

class UserConfirmationRequired(Exception):
    """Excepci√≥n personalizada para indicar que se requiere confirmaci√≥n del usuario."""
    def __init__(self, message: str):
        self.message = message
        super().__init__(self.message)

@dataclass
class AgentState:
    """Define la estructura del estado que fluye a trav√©s del grafo."""
    messages: List[BaseMessage] = field(default_factory=list)
    command_to_confirm: Optional[str] = None # Nuevo campo para comandos que requieren confirmaci√≥n
    tool_call_id_to_confirm: Optional[str] = None # Nuevo campo para el tool_call_id asociado al comando
    current_agent_mode: str = "bash" # A√±adido para el modo del agente
    
    # Nuevos campos para manejar la confirmaci√≥n de herramientas
    tool_pending_confirmation: Optional[str] = None
    tool_args_pending_confirmation: Optional[Dict[str, Any]] = None

    def reset_tool_confirmation(self):
        """Reinicia el estado de la confirmaci√≥n de herramientas."""
        self.tool_pending_confirmation = None
        self.tool_args_pending_confirmation = None
        self.tool_call_id_to_confirm = None # Asegurarse de limpiar tambi√©n este campo

    def reset(self):
        """Reinicia completamente el estado del agente."""
        self.messages = []
        self.command_to_confirm = None
        self.tool_call_id_to_confirm = None
        self.reset_tool_confirmation() # Reutilizar la l√≥gica de reinicio de confirmaci√≥n

    @property
    def history_for_api(self) -> list[BaseMessage]:
        """Devuelve el historial de mensajes de LangChain directamente."""
        return self.messages

# --- Nodos del Grafo ---

from rich.live import Live # Importar Live
from rich.markdown import Markdown # Importar Markdown
from rich.padding import Padding # Nueva importaci√≥n
from rich.status import Status # ¬°Nueva importaci√≥n!

def handle_tool_confirmation(state: AgentState, llm_service: LLMService):
    """
    Maneja la respuesta de confirmaci√≥n del usuario para una operaci√≥n de herramienta.
    Si se aprueba, re-ejecuta la herramienta.
    """
    last_message = state.messages[-1]
    if not isinstance(last_message, ToolMessage):
        # Esto no deber√≠a pasar si el flujo es correcto
        console.print("[bold red]Error: handle_tool_confirmation llamado sin un ToolMessage.[/bold red]")
        state.reset_tool_confirmation()
        return state

    tool_message_content = last_message.content
    tool_id = state.tool_call_id_to_confirm # Usar el tool_id guardado

    # Asumimos que el ToolMessage de confirmaci√≥n tiene un formato espec√≠fico
    # ej. "Confirmaci√≥n de usuario: Aprobado para 'escribir en el archivo ...'."
    if "Aprobado" in tool_message_content:
        console.print("[bold green]‚úÖ Confirmaci√≥n de usuario recibida: Aprobado.[/bold green]")
        tool_name = state.tool_pending_confirmation
        tool_args = state.tool_args_pending_confirmation

        if tool_name and tool_args:
            console.print(f"[bold blue]üõ†Ô∏è Re-ejecutando herramienta '{tool_name}' tras aprobaci√≥n:[/bold blue]")
            tool = llm_service.get_tool(tool_name)
            if tool:
                try:
                    if tool_name == "file_operations" and tool_args.get("operation") == "write_file":
                        tool_output_str = tool._perform_write_file(path=tool_args["path"], content=tool_args["content"])
                    elif tool_name == "file_operations" and tool_args.get("operation") == "delete_file":
                        tool_output_str = tool._perform_delete_file(path=tool_args["path"])
                    else:
                        tool_output_str = str(tool.invoke(tool_args))
                    tool_messages = [ToolMessage(content=tool_output_str, tool_call_id=tool_id)]
                    state.messages.extend(tool_messages)
                    console.print(f"[green]‚ú® Herramienta '{tool_name}' re-ejecutada con √©xito.[/green]")
                except Exception as e:
                    error_output = f"Error al re-ejecutar la herramienta {tool_name} tras aprobaci√≥n: {e}"
                    state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                    console.print(f"[bold red]‚ùå {error_output}[/bold red]")
            else:
                error_output = f"Error: Herramienta '{tool_name}' no encontrada para re-ejecuci√≥n."
                state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
                console.print(f"[bold red]‚ùå {error_output}[/bold red]")
        else:
            error_output = "Error: No se encontr√≥ informaci√≥n de la herramienta pendiente para re-ejecuci√≥n."
            state.messages.append(ToolMessage(content=error_output, tool_call_id=tool_id))
            console.print(f"[bold red]‚ùå {error_output}[/bold red]")
    else:
        console.print("[bold yellow]‚ö†Ô∏è Confirmaci√≥n de usuario recibida: Denegado.[/bold yellow]")
        tool_output_str = f"Operaci√≥n denegada por el usuario: {state.tool_pending_confirmation}"
        state.messages.append(ToolMessage(content=tool_output_str, tool_call_id=tool_id))

    state.reset_tool_confirmation() # Limpiar el estado de confirmaci√≥n
    state.tool_call_id_to_confirm = None # Limpiar tambi√©n el tool_call_id guardado
    return state

def call_model_node(state: AgentState, llm_service: LLMService):
    """Llama al LLM con el historial actual de mensajes y obtiene el resultado final, mostrando el streaming en Markdown."""
    history = state.history_for_api
    
    full_response_content = ""
    final_ai_message_from_llm = None
    text_streamed = False # Bandera para saber si hubo contenido de texto transmitido

    # Usar Live para actualizar el contenido en tiempo real
    with Live(console=console, screen=False, refresh_per_second=4) as live:
        for part in llm_service.invoke(history=history, system_message=SYSTEM_MESSAGE.content):
            if isinstance(part, AIMessage):
                final_ai_message_from_llm = part
                # Si el AIMessage final tiene contenido, lo a√±adimos al full_response_content
                if part.content:
                    full_response_content += part.content
            else:
                # Este 'part' es un chunk de texto (str)
                full_response_content += part
                text_streamed = True # Hubo streaming de texto
                # Actualizar el contenido de Live con el Markdown acumulado
                live.update(Padding(Markdown(full_response_content), (1, 4)))

    # --- L√≥gica del Agente despu√©s de recibir la respuesta completa del LLM ---

    # Si hubo tool_calls, el AIMessage ya los contendr√°.
    if final_ai_message_from_llm and final_ai_message_from_llm.tool_calls:
        # El AIMessage final para el historial debe contener el contenido completo
        # y los tool_calls.
        ai_message_for_history = AIMessage(content=full_response_content, tool_calls=final_ai_message_from_llm.tool_calls)
        
        state.messages.append(ai_message_for_history)
        
        # Si la herramienta es 'execute_command', establecemos command_to_confirm
        command_to_execute = None
        tool_call_id = None # Inicializar tool_call_id
        for tc in final_ai_message_from_llm.tool_calls:
            if tc['name'] == 'execute_command':
                command_to_execute = tc['args'].get('command')
                tool_call_id = tc['id'] # Capturar el tool_call_id
                break # Asumimos una sola llamada a comando por ahora

        return {
            "messages": state.messages,
            "command_to_confirm": command_to_execute, # Devolver el comando para confirmaci√≥n
            "tool_call_id_to_confirm": tool_call_id # Devolver el tool_call_id asociado
        }
    
    elif final_ai_message_from_llm: # Si es solo un AIMessage de texto (sin tool_calls)
        # El AIMessage final para el historial debe contener el contenido completo.
        ai_message_for_history = AIMessage(content=full_response_content)
        
        state.messages.append(ai_message_for_history)
        return {"messages": state.messages}
    else:
        # Fallback si por alguna raz√≥n no se obtuvo un AIMessage (poco probable con llm_service.py)
        error_message = "El modelo no proporcion√≥ una respuesta AIMessage v√°lida despu√©s de procesar los chunks."
        state.messages.append(AIMessage(content=error_message))
        return {"messages": state.messages}


def execute_tool_node(state: AgentState, llm_service: LLMService):
    """Ejecuta las herramientas solicitadas por el modelo."""
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:
        return state

    tool_messages = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call['name']
        tool_args = tool_call['args']
        tool_id = tool_call['id']

        # --- A√±adir logs para la ejecuci√≥n de herramientas ---
        console.print(f"\n[bold blue]üõ†Ô∏è Ejecutando herramienta:[/bold blue] [yellow]{tool_name}[/yellow]")
        
        tool = llm_service.get_tool(tool_name)
        if not tool:
            tool_output_str = f"Error: Herramienta '{tool_name}' no encontrada."
        else:
            try:
                raw_tool_output = tool.invoke(tool_args)
                tool_output_str = str(raw_tool_output) # Convertir a cadena
            except UserConfirmationRequired as e:
                # Si la herramienta requiere confirmaci√≥n, guardamos el estado y terminamos la ejecuci√≥n de herramientas.
                state.tool_pending_confirmation = tool_name
                state.tool_args_pending_confirmation = tool_args
                # El mensaje de la excepci√≥n se usar√° como confirmation_prompt en KogniTermApp
                state.tool_call_id_to_confirm = tool_id # Guardar el tool_id original
                tool_output_str = f"UserConfirmationRequired: {e.message}"
                console.print(f"[bold yellow]‚ö†Ô∏è Herramienta '{tool_name}' requiere confirmaci√≥n:[/bold yellow] {e.message}")
                return state # Salir de la ejecuci√≥n de herramientas, KogniTermApp manejar√° la confirmaci√≥n
            except Exception as e:
                tool_output_str = f"Error al ejecutar la herramienta {tool_name}: {e}"
        
        # --- Procesar y mostrar el mensaje descriptivo de la herramienta ---
        first_line = tool_output_str.split('\n')[0].strip()
        remaining_output = tool_output_str

        # Heur√≠stica simple para detectar un mensaje descriptivo
        is_descriptive_message = False
        if first_line and first_line.endswith('.') and len(first_line) > 10: # Si termina en punto y es lo suficientemente largo
            try:
                json.loads(first_line) # Intenta parsear como JSON, si falla, es probable que sea descriptivo
            except json.JSONDecodeError:
                is_descriptive_message = True
        
        if is_descriptive_message:
            console.print(f"[green]‚ú® {first_line}[/green]")
            remaining_output = '\n'.join(tool_output_str.split('\n')[1:]).strip() # Eliminar la primera l√≠nea
            if not remaining_output: # Si no queda m√°s contenido despu√©s del mensaje descriptivo
                remaining_output = "La herramienta se ejecut√≥ correctamente y no produjo m√°s salida."
        elif tool_output_str.strip() == "":
            remaining_output = "La herramienta se ejecut√≥ correctamente y no produjo ninguna salida."

        # --- Fin de procesamiento de mensaje descriptivo ---

        if tool_name == "execute_command":
            # Si es execute_command, establecer command_to_confirm y terminar el grafo aqu√≠.
            # La terminal se encargar√° de la confirmaci√≥n y ejecuci√≥n.
            state.command_to_confirm = tool_args['command']
            return state # Esto har√° que el grafo termine en este nodo si no hay m√°s tool_calls
        else:
            tool_messages.append(ToolMessage(content=remaining_output, tool_call_id=tool_id))

    state.messages.extend(tool_messages)
    return state

# --- L√≥gica Condicional del Grafo ---

def should_continue(state: AgentState) -> str:
    """Decide si continuar llamando a herramientas o finalizar."""
    last_message = state.messages[-1]
    
    # Si hay un comando pendiente de confirmaci√≥n, siempre terminamos el grafo aqu√≠
    # para que la terminal lo maneje.
    if state.command_to_confirm:
        return END

    # Si el √∫ltimo mensaje del AI tiene tool_calls, ejecutar herramientas
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "execute_tool"
    # Si el √∫ltimo mensaje es un ToolMessage (resultado de una herramienta),
    # volver a llamar al modelo para que genere una respuesta final.
    elif isinstance(last_message, ToolMessage):
        return "call_model"
    else:
        return END

# --- Construcci√≥n del Grafo ---

def create_bash_agent(llm_service: LLMService):
    bash_agent_graph = StateGraph(AgentState)

    bash_agent_graph.add_node("call_model", functools.partial(call_model_node, llm_service=llm_service))
    bash_agent_graph.add_node("execute_tool", functools.partial(execute_tool_node, llm_service=llm_service))

    bash_agent_graph.set_entry_point("call_model")

    bash_agent_graph.add_conditional_edges(
        "call_model",
        should_continue,
        {
            "execute_tool": "execute_tool",
            END: END
        }
    )

    bash_agent_graph.add_edge("execute_tool", "call_model")

    return bash_agent_graph.compile()
