import sys
from langgraph.graph import StateGraph, END
from dataclasses import dataclass, field
from typing import List, Optional

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
import google.generativeai as genai
from google.generativeai.protos import Part, FunctionCall

from kogniterm.core.llm_service import LLMService

# Inicializar el servicio LLM de forma global
llm_service = LLMService()

# --- Mensaje de Sistema ---
SYSTEM_MESSAGE = SystemMessage(content="""¬°Hola! üëã Soy KogniTerm, tu asistente de IA experto en terminal. ¬°Estoy aqu√≠ para ayudarte a realizar tareas directamente en tu sistema de la manera m√°s amigable y eficiente posible! ‚ú®

Cuando me pidas algo, yo me encargar√© de ejecutarlo por ti. Mi objetivo es simplificar tu trabajo y que la experiencia sea genial. üöÄ

Aqu√≠ te explico c√≥mo funciono y qu√© espero de ti:

1.  **Analiza la petici√≥n**: Entiendo perfectamente lo que quieres lograr. ¬°No te preocupes!
2.  **Usa mis herramientas**: Tengo un conjunto de herramientas super√∫tiles a mi disposici√≥n, como `execute_command` para comandos de terminal, y otras para buscar en la web (`brave_search`) y acceder a contenido de URLs (`web_fetch`). ¬°Las usar√© sin dudar para completar tus tareas! üõ†Ô∏è
3.  **Ejecuta directamente**: ¬°No te pedir√© que ejecutes comandos! Yo los har√© por ti usando la herramienta `execute_command`. As√≠, t√∫ te relajas y yo hago el trabajo pesado. üòâ
4.  **Informa del resultado**: Una vez que la tarea est√© lista, te informar√© el resultado de forma clara, concisa y con una sonrisa. ¬°Siempre con un toque amigable! üòä

La herramienta `execute_command` maneja la interactividad y seguridad de los comandos, as√≠ que puedes confiar en que todo estar√° bien.

**¬°IMPORTANTE!** üö® Cuando use la herramienta `execute_command`, no la combinar√© con otras herramientas en la misma respuesta. ¬°`execute_command` ser√° la √∫nica herramienta en esa llamada!

Cuando reciba la salida de una herramienta, la analizar√©, resumir√© y te la presentar√© de forma clara y amigable, usando Markdown si es necesario para que todo sea f√°cil de leer. üìù

¬°Cuento con tu permiso para operar en tu sistema! Actuar√© de forma proactiva para completar tus peticiones. ¬°Vamos a hacer cosas incre√≠bles juntos! üí°

**Gesti√≥n de Memoria:**
*   **Inicio de Conversaci√≥n**: Al inicio de cada conversaci√≥n, verificar√© si el archivo de memoria existe. Si no existe, lo inicializar√© autom√°ticamente usando `memory_init_tool`. Si existe, lo leer√© usando `memory_read_tool` para cargar el contexto previo. Esto me permite recordar contextos importantes entre sesiones. üß†
*   **Guardado Aut√≥nomo**: Guardar√© memorias relevantes de forma aut√≥noma cuando me plantees temas importantes como detalles del directorio actual, objetivos del proyecto o cualquier informaci√≥n crucial. Utilizar√© `memory_append_tool` para asegurarme de que esta informaci√≥n est√© siempre disponible cuando la necesite. üíæ
*   **Acceso a Memoria**: Puedo acceder a la informaci√≥n guardada en tu memoria en cualquier momento usando `memory_read_tool` para ayudarte de manera m√°s eficiente y contextualizada. üìñ
""")

# --- Definici√≥n del Estado del Agente ---

@dataclass
class AgentState:
    """Define la estructura del estado que fluye a trav√©s del grafo."""
    messages: List[BaseMessage] = field(default_factory=lambda: [SYSTEM_MESSAGE])
    command_to_confirm: Optional[str] = None # Nuevo campo para comandos que requieren confirmaci√≥n
    action_needed: Optional[str] = None # A√±adido para consistencia con OrchestratorState

    @property
    def history_for_api(self) -> list[dict]:
        """Convierte los mensajes de LangChain al formato que espera la API de Google AI."""
        api_history = []
        messages = self.messages
        i = 0
        while i < len(messages):
            msg = messages[i]
            if isinstance(msg, ToolMessage):
                tool_messages_buffer = []
                # Collect all consecutive ToolMessages
                while i < len(messages) and isinstance(messages[i], ToolMessage):
                    tool_messages_buffer.append(messages[i])
                    i += 1
                
                parts = [
                    Part(function_response=genai.protos.FunctionResponse(
                        name=tm.tool_call_id,
                        response={'content': tm.content}
                    )) for tm in tool_messages_buffer
                ]
                api_history.append({'role': 'user', 'parts': parts})
                continue # Continue to the next message after the buffer

            if isinstance(msg, (HumanMessage, SystemMessage)):
                api_history.append({'role': 'user', 'parts': [Part(text=msg.content)]})
            elif isinstance(msg, AIMessage):
                if msg.tool_calls:
                    parts = [
                        Part(function_call=FunctionCall(name=tc['name'], args=tc['args']))
                        for tc in msg.tool_calls
                    ]
                    api_history.append({'role': 'model', 'parts': parts})
                else:
                    api_history.append({'role': 'model', 'parts': [Part(text=msg.content)]})
            i += 1
        return api_history

# --- Nodos del Grafo ---

async def call_model_node(state: AgentState):
    """Llama al LLM con el historial actual de mensajes."""
    history = state.history_for_api
    response = await llm_service.ainvoke(history)
    
    # Check if response has candidates and parts
    if response.candidates and response.candidates[0].content.parts:
        # Iterate through all parts to find tool calls
        tool_calls = []
        text_response_parts = []
        for part in response.candidates[0].content.parts:
            if part.function_call.name:
                tool_name = part.function_call.name
                tool_args = {key: value for key, value in part.function_call.args.items()}
                tool_calls.append({"name": tool_name, "args": tool_args, "id": tool_name})
            elif part.text:
                text_response_parts.append(part.text)
        
        if tool_calls:
            ai_message = AIMessage(content="", tool_calls=tool_calls)
            state.messages.append(ai_message)
        elif text_response_parts:
            text_response = "".join(text_response_parts)
            state.messages.append(AIMessage(content=text_response))
    else:
        # Handle cases where there's no content (e.g., blocked response)
        # The llm_service.invoke should return an error message in this case
        error_message = "El modelo no proporcion√≥ una respuesta v√°lida."
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            error_message += f" Raz√≥n de bloqueo: {response.prompt_feedback.block_reason.name}"
        state.messages.append(AIMessage(content=error_message))
    
    return state

async def explain_command_node(state: AgentState):
    """Genera una explicaci√≥n en lenguaje natural del comando a ejecutar."""
    # El √∫ltimo mensaje del AI es la llamada a herramienta para execute_command
    last_ai_message = state.messages[-1]
    command = last_ai_message.tool_calls[0].args['command']

    explanation_prompt = f"El siguiente comando ser√° ejecutado: `{command}`. Por favor, explica en lenguaje natural qu√© har√° este comando y por qu√© es necesario para la tarea actual. S√© conciso y claro."
    
    # Llamar al modelo para obtener una explicaci√≥n
    # Necesitamos una copia del historial sin la √∫ltima llamada a herramienta para que el modelo genere texto
    temp_history = state.history_for_api[:-1] # Eliminar el √∫ltimo mensaje del AI con la llamada a herramienta
    temp_history.append({'role': 'user', 'parts': [explanation_prompt]}) # A√±adir el prompt de explicaci√≥n
    
    response = await llm_service.ainvoke(temp_history)
    explanation_text = response.candidates[0].content.parts[0].text

    # A√±adir la explicaci√≥n a los mensajes
    state.messages.append(AIMessage(content=explanation_text))
    
    # Ahora establecer el comando para confirmar
    state.command_to_confirm = command
    return state

async def execute_tool_node(state: AgentState):
    """Ejecuta las herramientas solicitadas por el modelo."""
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:
        return state

    tool_messages = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call['name']
        tool_args = tool_call['args']
        tool_id = tool_call['id']

        if tool_name == "execute_command":
            # A√±adir un ToolMessage para mantener la consistencia en el historial
            tool_messages.append(ToolMessage(content="Comando enviado para explicaci√≥n y confirmaci√≥n.", tool_call_id=tool_id))
            # El agente transicionar√° a explain_command_node
            return state
        else:
            # Imprimir un mensaje para notificar al usuario que la herramienta se est√° ejecutando
            # Esto es √∫til para herramientas que pueden tardar un poco.
            print(f"‚öôÔ∏è  Ejecutando herramienta: {tool_name}...", file=sys.stdout, flush=True)
            
            tool = llm_service.get_tool(tool_name)
            if not tool:
                tool_output = f"Error: Herramienta '{tool_name}' no encontrada."
            else:
                try:
                    # Usar ainvoke para la ejecuci√≥n as√≠ncrona de la herramienta
                    tool_output = await tool.ainvoke(tool_args)
                    # Limitar el tama√±o de la salida de la herramienta a 1000 caracteres
                    # para evitar exceder el l√≠mite de payload de la API de Gemini
                    if len(str(tool_output)) > 1000:
                        tool_output = str(tool_output)[:997] + "..."
                except Exception as e:
                    tool_output = f"Error al ejecutar la herramienta {tool_name}: {e}"
            
            # Imprime una nueva l√≠nea para separar la notificaci√≥n de la salida final.
            print()
            
            tool_messages.append(ToolMessage(content=str(tool_output), tool_call_id=tool_id))

    state.messages.extend(tool_messages)
    return state

# --- L√≥gica Condicional del Grafo ---

def should_continue(state: AgentState) -> str:
    """Decide si continuar llamando a herramientas o finalizar."""
    last_message = state.messages[-1]
    if state.command_to_confirm: # Si hay un comando para confirmar, necesitamos ir a un paso de confirmaci√≥n
        return "confirm_command"
    elif isinstance(last_message, AIMessage) and last_message.tool_calls:
        # Si es una llamada a herramienta execute_command, ir a explain_command
        if last_message.tool_calls[0]['name'] == "execute_command":
            return "explain_command"
        else:
            return "execute_tool"
    else:
        return END

# --- Construcci√≥n del Grafo ---

bash_agent_graph = StateGraph(AgentState)

bash_agent_graph.add_node("call_model", call_model_node)
bash_agent_graph.add_node("execute_tool", execute_tool_node)
bash_agent_graph.add_node("explain_command", explain_command_node) # Nuevo nodo
bash_agent_graph.add_node("confirm_command", lambda state: state) # Nuevo nodo, solo pasa el estado

bash_agent_graph.set_entry_point("call_model")

bash_agent_graph.add_conditional_edges(
    "call_model",
    should_continue,
    {
        "execute_tool": "execute_tool",
        "explain_command": "explain_command", # Nueva transici√≥n
        "confirm_command": "confirm_command",
        END: END
    }
)

bash_agent_graph.add_edge("execute_tool", "call_model")
bash_agent_graph.add_edge("explain_command", "confirm_command") # Nueva transici√≥n
bash_agent_graph.add_edge("confirm_command", END) # El agente termina aqu√≠, la terminal toma el control

bash_agent_app = bash_agent_graph.compile()
