#!/usr/bin/env python3
"""
Script de prueba para verificar que la correcciÃ³n del resumen automÃ¡tico funciona correctamente.
"""

import sys
import os
sys.path.append('/home/gato/Gemini-Interpreter')

from kogniterm.core.llm_service import LLMService

def test_summary_length_limit():
    """Prueba que el lÃ­mite de longitud del resumen funciona correctamente."""

    # Crear una instancia del servicio
    service = LLMService()

    # Simular un resumen muy largo (como el que causaba el problema)
    long_summary = "Este es un resumen muy extenso que contiene mucha informaciÃ³n detallada sobre la conversaciÃ³n anterior. " * 100
    print(f"Longitud del resumen original: {len(long_summary)} caracteres")

    # Aplicar la lÃ³gica de lÃ­mite de longitud
    max_summary_length = min(2000, service.max_history_chars // 4)
    print(f"LÃ­mite mÃ¡ximo de resumen: {max_summary_length} caracteres")

    if len(long_summary) > max_summary_length:
        truncated_summary = long_summary[:max_summary_length] + "... [Resumen truncado para evitar bucles]"
        print(f"Longitud del resumen truncado: {len(truncated_summary)} caracteres")
        print(f"Resumen truncado correctamente: {'âœ…' if len(truncated_summary) <= max_summary_length + 50 else 'âŒ'}")
        return True
    else:
        print("El resumen no necesitaba truncarse")
        return True

def test_prompt_improvement():
    """Prueba que el prompt mejorado es mÃ¡s conciso."""

    # Prompt anterior (extenso)
    old_prompt = "Por favor, resume la siguiente conversaciÃ³n de manera EXHAUSTIVA, DETALLADA y EXTENSA. Captura todos los puntos clave, decisiones tomadas, tareas pendientes y cualquier informaciÃ³n relevante que pueda ser Ãºtil para retomar la conversaciÃ³n con el contexto completo. El resumen debe ser lo mÃ¡s completo posible y actuar como un reemplazo fiel del historial para la comprensiÃ³n del LLM en el futuro."

    # Prompt nuevo (conciso)
    new_prompt = "Por favor, resume la siguiente conversaciÃ³n de manera CONCISA pero COMPLETA. Captura los puntos clave, decisiones tomadas, tareas pendientes y contexto esencial. Limita el resumen a mÃ¡ximo 1500 caracteres para evitar problemas de longitud. SÃ© especÃ­fico pero econÃ³mico con las palabras."

    print(f"Longitud del prompt anterior: {len(old_prompt)} caracteres")
    print(f"Longitud del prompt nuevo: {len(new_prompt)} caracteres")
    print(f"Prompt mejorado (mÃ¡s corto): {'âœ…' if len(new_prompt) < len(old_prompt) else 'âŒ'}")

    return len(new_prompt) < len(old_prompt)

if __name__ == "__main__":
    print("ðŸ§ª Probando correcciÃ³n del problema de resumen automÃ¡tico...")
    print()

    print("1. Prueba del lÃ­mite de longitud del resumen:")
    test1_passed = test_summary_length_limit()
    print()

    print("2. Prueba de la mejora del prompt:")
    test2_passed = test_prompt_improvement()
    print()

    if test1_passed and test2_passed:
        print("ðŸŽ‰ Todas las pruebas pasaron! La correcciÃ³n deberÃ­a resolver el problema.")
        print()
        print("Resumen de la correcciÃ³n:")
        print("- âœ… Se agregÃ³ lÃ­mite de longitud para resÃºmenes (mÃ¡ximo 2000 caracteres)")
        print("- âœ… Se mejorÃ³ el prompt para generar resÃºmenes mÃ¡s concisos")
        print("- âœ… Se evita el bucle infinito de resÃºmenes automÃ¡ticos")
    else:
        print("âŒ Algunas pruebas fallaron. Revisar la implementaciÃ³n.")
