# üìù Estado de la Implementaci√≥n del Sistema RAG en KogniTerm

Este documento resume el progreso realizado en la integraci√≥n del sistema RAG (Retrieval-Augmented Generation) en KogniTerm hasta la fecha.

## üåü Visi√≥n General del Proyecto

El objetivo es integrar un sistema RAG de codebase en KogniTerm que permita:
1.  **Configurar** proveedores y modelos de embeddings desde la terminal.
2.  **Indexar** el c√≥digo fuente de un proyecto en una base de datos vectorial local.
3.  **Recuperar** fragmentos de c√≥digo relevantes para enriquecer el contexto del LLM.
4.  Funcionar de manera **autocontenida** por proyecto/directorio.

## ‚úÖ Fases Completadas

Hemos completado exitosamente las Fases 1 a 5 del plan de implementaci√≥n.

### FASE 1: ‚öôÔ∏è Configuraci√≥n CLI y Gesti√≥n de Archivos de Configuraci√≥n
*   **`kogniterm/core/config.py`**:
    *   Se a√±adieron campos de configuraci√≥n espec√≠ficos para RAG (`embeddings_provider`, `embeddings_model`, `codebase_index_exclude_dirs`, `codebase_index_include_patterns`, `codebase_chunk_size`, `codebase_chunk_overlap`).
    *   Se implement√≥ la l√≥gica para cargar configuraciones desde archivos JSON globales (`~/.kogniterm/config.json`) y por proyecto (`.kogniterm/config.json`), utilizando `pydantic-settings` y una fuente de configuraci√≥n JSON personalizada.
    *   Se a√±adieron m√©todos `save_project_config()` y `save_global_config()` para persistir las configuraciones.
*   **`kogniterm/terminal/meta_command_processor.py`**:
    *   Se modific√≥ para manejar los comandos CLI `%config set <clave> <valor>` y `%config project set <clave> <valor>`, permitiendo a los usuarios establecer y persistir configuraciones.
    *   Se actualiz√≥ el mensaje de ayuda (`%help`) para incluir estos nuevos comandos.

### FASE 2: üåê Servicio de Embeddings y Abstracci√≥n de Proveedores
*   **`kogniterm/core/embeddings_service.py`**:
    *   Se cre√≥ un nuevo m√≥dulo con la clase `EmbeddingsService`.
    *   Esta clase proporciona una interfaz unificada para generar embeddings utilizando `litellm`, soportando proveedores como OpenAI, Google Gemini y Ollama, y leyendo la configuraci√≥n desde `settings`.
*   **`kogniterm/core/llm_service.py`**:
    *   Se integr√≥ el `EmbeddingsService` en el constructor de `LLMService`, asegurando que el servicio de embeddings est√© disponible para otros componentes.

### FASE 3: üìö Indexador de Codebase (Chunking y Embedding)
*   **`kogniterm/core/context/codebase_indexer.py`**:
    *   Se cre√≥ un nuevo m√≥dulo con la clase `CodebaseIndexer`.
    *   Implementa la l√≥gica para recorrer el directorio del proyecto, listar archivos de c√≥digo (respetando patrones de inclusi√≥n/exclusi√≥n de `settings`).
    *   Divide los archivos en "chunks" l√≥gicos con solapamiento, utilizando `settings.codebase_chunk_size` y `settings.codebase_chunk_overlap`.
    *   Orquesta la generaci√≥n de embeddings para cada chunk utilizando el `EmbeddingsService`.

### FASE 4: üìä Gesti√≥n de la Base de Datos Vectorial (ChromaDB)
*   **`kogniterm/core/context/vector_db_manager.py`**:
    *   Se cre√≥ un nuevo m√≥dulo con la clase `VectorDBManager`.
    *   Gestiona la inicializaci√≥n de una instancia de ChromaDB en modo persistente para cada proyecto (en `.kogniterm/vector_db/`).
    *   Proporciona m√©todos para a√±adir chunks y sus embeddings a la base de datos, y para realizar b√∫squedas de similitud (`search`).

### FASE 5: üîç Herramienta de Recuperaci√≥n (Retriever Tool)
*   **`kogniterm/core/tools/codebase_search_tool.py`**:
    *   Se cre√≥ un nuevo m√≥dulo con la clase `CodebaseSearchTool`, que hereda de `BaseTool` de LangChain.
    *   Permite al agente realizar b√∫squedas de fragmentos de c√≥digo relevantes en la base de datos vectorial del proyecto.
    *   Utiliza el `EmbeddingsService` para generar embeddings de la consulta y el `VectorDBManager` para la b√∫squeda.
    *   Formatea los resultados de la b√∫squeda en una cadena de texto √∫til para el contexto del LLM.

## üöß Fases Pendientes (Fase 6: Integraci√≥n y Flujo de Trabajo RAG)

Actualmente estamos trabajando en la Fase 6, que es la integraci√≥n final de todos los componentes.

### Tareas Pendientes:

1.  **Integraci√≥n en `kogniterm/terminal/terminal.py` y `kogniterm/terminal/kogniterm_app.py`**:
    *   **Detecci√≥n y solicitud de indexaci√≥n al inicio**: Al iniciar KogniTerm en un proyecto, si no hay un √≠ndice de codebase, se le preguntar√° al usuario si desea crearlo. Esta l√≥gica ya est√° parcialmente implementada en `_main_async()` en `kogniterm/terminal/terminal.py`.
    *   **Pendiente de corregir la firma del `__init__` de `KogniTermApp` y pasar `codebase_indexer` y `vector_db_manager`**: Hubo problemas recurrentes con la edici√≥n de la firma del `__init__` en `kogniterm/terminal/kogniterm_app.py` y la inicializaci√≥n de `MetaCommandProcessor` debido a fallos con `replace_regex`. Esto necesita una correcci√≥n precisa.

2.  **Integraci√≥n de RAG en el agente (modificar `kogniterm/core/llm_service.py` o la l√≥gica de toma de decisiones del agente)**:
    *   Se necesita implementar la l√≥gica para que el agente identifique cu√°ndo una consulta del usuario o una tarea podr√≠a beneficiarse de la recuperaci√≥n de c√≥digo.
    *   El agente deber√° invocar la `codebase_search_tool` con una consulta relevante.
    *   Los resultados de la b√∫squeda de la herramienta se inyectar√°n en el contexto del LLM (como parte del `SystemMessage` o un `ToolMessage`) antes de generar la respuesta final.

## ‚ö†Ô∏è Problemas Recurrentes Identificados

*   **Error de "M√°ximo de Tokens por Minuto"**: Se ha reportado un error recurrente de l√≠mite de tokens por minuto. Esto sugiere la necesidad de una gesti√≥n de tasas m√°s robusta o ajustes en la configuraci√≥n de `litellm`.
*   **Error en la Compresi√≥n del Historial**: Se ha reportado un problema con la funci√≥n de compresi√≥n del historial. Esto necesita depuraci√≥n en `kogniterm/core/llm_service.py` (m√©todo `summarize_conversation_history`).
*   **Problemas con `replace_regex` en `advanced_file_editor`**: Se han experimentado dificultades para aplicar cambios precisos a l√≠neas espec√≠ficas usando `replace_regex`, lo que ha llevado a errores de sintaxis y a la imposibilidad de aplicar modificaciones cruciales. Esto ha sido el principal obst√°culo para la integraci√≥n de los par√°metros RAG en `KogniTermApp`.

## Pr√≥ximos Pasos

1.  **Corregir la firma del `__init__` en `kogniterm/terminal/kogniterm_app.py` y la inicializaci√≥n de `MetaCommandProcessor`**: Utilizar una estrategia de edici√≥n m√°s robusta (lectura completa, modificaci√≥n en memoria, reescritura) para asegurar que estos cambios se apliquen correctamente.
2.  **Abordar el error de "M√°ximo de Tokens por Minuto"**: Investigar opciones de `rate limiting` en `litellm` o implementar pausas expl√≠citas.
3.  **Depurar el error de la Compresi√≥n del Historial**: Revisar `summarize_conversation_history` en `kogniterm/core/llm_service.py`.
4.  Una vez resueltos los problemas anteriores, continuar con la **integraci√≥n de RAG en el agente** en `kogniterm/core/llm_service.py` o la l√≥gica de toma de decisiones del agente.
