# üìù Propuesta: Implementaci√≥n de un Sistema RAG de Codebase en KogniTerm

## 1. Introducci√≥n: Potenciando el Contexto de KogniTerm con RAG

El sistema RAG (Retrieval-Augmented Generation) para el codebase permitir√° a KogniTerm acceder y recuperar din√°micamente fragmentos de c√≥digo relevantes del proyecto actual, utilizando estos fragmentos como contexto adicional para el LLM. Esto mejorar√° dr√°sticamente la precisi√≥n, relevancia y eficiencia de las respuestas del agente, especialmente en tareas de depuraci√≥n, generaci√≥n de c√≥digo y an√°lisis.

Nos basaremos en el concepto de "√≠ndice de la base de c√≥digo" de Kilo Code, pero adaptado a la naturaleza empaquetada y multicontexto de KogniTerm.

## 2. Componentes Clave de un Sistema RAG de Codebase para KogniTerm

Para implementar un sistema RAG robusto, necesitaremos los siguientes componentes:

### 2.1. üìö Indexador de Codebase

*   **Funci√≥n:** Recorre el directorio del proyecto, identifica archivos de c√≥digo relevantes, los divide en "chunks" (fragmentos l√≥gicos como funciones, clases, bloques de c√≥digo) y genera embeddings vectoriales para cada chunk.
*   **Archivos a Indexar:** Podr√≠a configurarse para incluir archivos `.py`, `.js`, `.ts`, `.java`, `.c`, `.cpp`, `.go`, `.md`, `.json`, etc., y excluir directorios como `node_modules`, `.git`, `__pycache__`, etc.
*   **Ubicaci√≥n:** Un nuevo m√≥dulo en `kogniterm/core/context/codebase_indexer.py`.

### 2.2. üìä Base de Datos Vectorial (Vector DB)

*   **Funci√≥n:** Almacena los embeddings vectoriales generados por el indexador junto con metadatos asociados (ruta del archivo, contenido original del chunk, l√≠nea de inicio, l√≠nea de fin).
*   **Caracter√≠sticas Clave:** Debe ser ligera, f√°cil de integrar en una aplicaci√≥n empaquetada y capaz de soportar m√∫ltiples bases de datos por proyecto.
*   **Ubicaci√≥n:** Un nuevo m√≥dulo de abstracci√≥n en `kogniterm/core/context/vector_db_manager.py` que interact√∫e con la base de datos elegida.

### 2.3. üåê Proveedor de Embeddings

*   **Funci√≥n:** Una API o un modelo local que convierte texto (chunks de c√≥digo) en vectores num√©ricos de alta dimensi√≥n.
*   **Flexibilidad:** Debe ser configurable para permitir diferentes proveedores (OpenAI, Gemini, Ollama, Hugging Face, etc.).
*   **Ubicaci√≥n:** Extender `kogniterm/core/llm_service.py` o crear un `kogniterm/core/embeddings_service.py`.

### 2.4. üîç Recuperador (Retriever)

*   **Funci√≥n:** Dadas una consulta (pregunta del usuario, contexto actual del agente), genera un embedding para la consulta y lo utiliza para buscar los chunks de c√≥digo m√°s sem√°nticamente similares en la Base de Datos Vectorial.
*   **Ubicaci√≥n:** Una nueva herramienta `codebase_search_tool.py` en `kogniterm/core/tools/`.

## 3. ‚öôÔ∏è Opciones de Configuraci√≥n en la Terminal (CLI)

La configuraci√≥n deber√≠a ser intuitiva y flexible, permitiendo al usuario definir sus preferencias para cada proyecto o globalmente.

### 3.1. Comando de Configuraci√≥n Global y por Proyecto

Podr√≠amos introducir un comando `kogniterm config` o `kogniterm settings` con subcomandos:

```bash
kogniterm config set embeddings_provider openai
kogniterm config set embeddings_model text-embedding-ada-002
kogniterm config set openai_api_key sk-...

# Configuraci√≥n espec√≠fica para el proyecto actual
kogniterm config project set codebase_index_exclude_dirs "node_modules,dist"
```

*   **Implementaci√≥n:**
    *   Un nuevo m√≥dulo `kogniterm/terminal/config_manager.py` para manejar la lectura/escritura de configuraciones.
    *   La configuraci√≥n global se guardar√≠a en un archivo de configuraci√≥n en el directorio de usuario (ej., `~/.kogniterm/config.json`).
    *   La configuraci√≥n por proyecto se guardar√≠a en un archivo `.kogniterm/config.json` dentro del directorio del proyecto, sobrescribiendo la global si es necesario.

### 3.2. Configuraci√≥n de Proveedores y Modelos de Embeddings

*   **Proveedor:** Una lista de opciones predefinidas (OpenAI, Gemini, Ollama, etc.).
*   **Modelo:** Para cada proveedor, una lista de modelos compatibles.
*   **API Keys:** Gesti√≥n segura de claves API (variables de entorno, archivo de configuraci√≥n encriptado o prompts interactivos).

### 3.3. Configuraci√≥n del Indexador

*   **Archivos a incluir/excluir:** Patrones glob para incluir/excluir archivos y directorios durante la indexaci√≥n.
*   **Tama√±o de Chunk:** Configurar el tama√±o m√°ximo de los fragmentos de c√≥digo.
*   **Estrategia de Chunking:** (Opcional) Definir c√≥mo se dividen los archivos (por funci√≥n, por clase, por l√≠neas, etc.).

## 4. üóÑÔ∏è Estrategias de Gesti√≥n de la Base de Datos Vectorial

Dado que KogniTerm es una aplicaci√≥n empaquetada y cada directorio es un proyecto independiente, necesitamos una soluci√≥n de base de datos vectorial que sea ligera, integrada y local.

### 4.1. Opci√≥n 1: Base de Datos Vectorial Integrada y Basada en Archivos (Recomendada)

*   **Base de Datos Sugeridas:**
    *   **ChromaDB (modo persistente):** Es una base de datos vectorial de c√≥digo abierto, ligera y que puede funcionar completamente basada en archivos. Es f√°cil de instalar y gestionar.
    *   **FAISS (Facebook AI Similarity Search):** Una biblioteca para la b√∫squeda eficiente de similitud de vectores. Requiere un poco m√°s de gesti√≥n para la persistencia, pero es extremadamente r√°pida para la b√∫squeda.
*   **Estrategia por Proyecto:**
    *   Cada directorio de proyecto tendr√≠a su propia instancia de la Base de Datos Vectorial persistente.
    *   Se crear√≠a una subcarpeta oculta, por ejemplo, `.kogniterm/vector_db/`, dentro de cada directorio de proyecto. Aqu√≠ se almacenar√≠an los archivos de la Base de Datos Vectorial (ej., los archivos de ChromaDB).
    *   Esto garantiza el aislamiento total del contexto entre proyectos y facilita el borrado o la copia de proyectos.
*   **Ventajas:**
    *   **Portabilidad:** El directorio del proyecto es autocontenido, f√°cil de mover o compartir.
    *   **Aislamiento:** Un proyecto no interfiere con el √≠ndice de otro.
    *   **Facilidad de Instalaci√≥n:** No requiere servidores externos ni configuraciones complejas por parte del usuario.
*   **Desventajas:** El rendimiento podr√≠a ser un factor en proyectos *extremadamente* grandes (aunque poco probable para la mayor√≠a de los casos de uso).

### 4.2. Opci√≥n 2: Base de Datos Vectorial Ligera en Proceso (Menos Recomendada para Persistencia)

*   **Base de Datos Sugeridas:** `Annoy` (Approximate Nearest Neighbors Oh Yeah), `Hnswlib`.
*   **Estrategia:** La Base de Datos Vectorial se cargar√≠a en memoria al iniciar KogniTerm en un proyecto y se descartar√≠a al finalizar. Para la persistencia, los embeddings y metadatos se guardar√≠an en archivos JSON o SQLite y se recargar√≠an.
*   **Ventajas:** Muy r√°pida en memoria.
*   **Desventajas:** Gesti√≥n de persistencia manual, potencialmente m√°s lenta para cargar/guardar, mayor consumo de RAM para proyectos grandes si no se gestiona cuidadosamente.

## 5. üîÑ Flujo de Trabajo del Sistema RAG

1.  **Inicializaci√≥n del Proyecto:**
    *   Al abrir KogniTerm en un nuevo directorio de proyecto, se detecta la ausencia del √≠ndice de codebase.
    *   KogniTerm pregunta al usuario si desea indexar el proyecto (o lo hace autom√°ticamente si est√° configurado).
    *   El `Codebase Indexer` se ejecuta, genera embeddings y los almacena en la `Vector DB` local (`.kogniterm/vector_db/`).
2.  **Actualizaci√≥n del √çndice:**
    *   Se implementa un mecanismo para detectar cambios en los archivos (observador de archivos como `watchdog`) o un comando manual `kogniterm index refresh`.
    *   Solo se reindexan los archivos modificados o nuevos, para mayor eficiencia.
3.  **Consulta RAG (durante la interacci√≥n del agente):**
    *   Cuando el agente necesita contexto de c√≥digo (ej., para responder una pregunta sobre una funci√≥n, depurar un error), utiliza la herramienta `codebase_search`.
    *   El `Recuperador` genera un embedding de la consulta y busca los `N` chunks de c√≥digo m√°s relevantes en la `Vector DB` del proyecto.
    *   Los chunks recuperados se a√±aden al contexto del LLM como parte del `SystemMessage` o en un formato estructurado.
    *   El LLM utiliza este contexto de c√≥digo para generar una respuesta m√°s informada.

## 6. üõ†Ô∏è Consideraciones de Implementaci√≥n

*   **Dependencias:** Asegurarse de que las dependencias de la Base de Datos Vectorial (ej., `chromadb`) sean f√°ciles de instalar para el usuario final (posiblemente incluy√©ndolas en `requirements.txt` o como una dependencia opcional).
*   **Manejo de Errores:** Robustecer la indexaci√≥n para manejar archivos corruptos o errores en la generaci√≥n de embeddings.
*   **Rendimiento:** Optimizar el proceso de indexaci√≥n para proyectos grandes (indexaci√≥n incremental, procesamiento en segundo plano).
*   **Seguridad:** Si se usa un proveedor de embeddings en la nube, asegurar que las claves API se manejen de forma segura y nunca se expongan.
*   **UX/UI:** Proporcionar retroalimentaci√≥n clara al usuario durante la indexaci√≥n y la recuperaci√≥n.

## 7. Conclusi√≥n

La implementaci√≥n de un sistema RAG de codebase transformar√≠a a KogniTerm en un asistente mucho m√°s competente y aut√≥nomo en el manejo de proyectos de c√≥digo. Al combinar la configuraci√≥n flexible en la terminal con una gesti√≥n de base de datos vectorial local y por proyecto, aseguramos una soluci√≥n potente, integrada y f√°cil de usar.
