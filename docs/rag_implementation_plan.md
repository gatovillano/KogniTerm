# üó∫Ô∏è Plan de Implementaci√≥n RAG de Codebase para KogniTerm (con Agentes)

Este plan desglosa la implementaci√≥n del sistema RAG en KogniTerm en fases manejables, cada una con un objetivo claro y un prompt dise√±ado para el orquestador de Kilo Code.

## üåü Visi√≥n General del Proyecto

El objetivo es integrar un sistema RAG de codebase en KogniTerm que permita:
1.  **Configurar** proveedores y modelos de embeddings desde la terminal.
2.  **Indexar** el c√≥digo fuente de un proyecto en una base de datos vectorial local.
3.  **Recuperar** fragmentos de c√≥digo relevantes para enriquecer el contexto del LLM.
4.  Funcionar de manera **autocontenida** por proyecto/directorio.

---

## FASE 1: ‚öôÔ∏è Configuraci√≥n CLI y Gesti√≥n de Archivos de Configuraci√≥n

**Objetivo:** Implementar la l√≥gica para que KogniTerm pueda guardar y cargar configuraciones globales y por proyecto desde la terminal.

**Tareas Clave:**
*   Crear un m√≥dulo `config_manager.py` para manejar la lectura/escritura de configuraciones JSON.
*   Implementar comandos CLI para `kogniterm config set <clave> <valor>` y `kogniterm config project set <clave> <valor>`.
*   Asegurar que la configuraci√≥n por proyecto sobrescriba la global.
*   Manejo b√°sico de claves API (ej., guardar en el archivo de config o sugerir variables de entorno).

**M√≥dulos Involucrados:**
*   `kogniterm/terminal/config_manager.py` (nuevo)
*   Posiblemente modificar `kogniterm/main.py` para integrar los comandos CLI.

**Prompt para el Orquestador de Kilo Code (Fase 1):**

```
"Implementa la Fase 1 del sistema RAG de codebase para KogniTerm. El objetivo es crear un sistema robusto de gesti√≥n de configuraci√≥n CLI. Necesitas desarrollar un nuevo m√≥dulo `kogniterm/terminal/config_manager.py` que se encargue de leer y escribir configuraciones en formato JSON. Debe soportar configuraciones globales (en `~/.kogniterm/config.json`) y configuraciones espec√≠ficas por proyecto (en `.kogniterm/config.json` dentro del directorio actual del proyecto), donde las configuraciones del proyecto sobrescriben las globales.

Adem√°s, integra comandos CLI `kogniterm config set <clave> <valor>` y `kogniterm config project set <clave> <valor>` para permitir al usuario establecer estas configuraciones. Aseg√∫rate de que el manejo de claves API sea seguro, sugiriendo el uso de variables de entorno o guard√°ndolas de forma b√°sica en el archivo de configuraci√≥n.

Considera la estructura de directorios existente de KogniTerm y c√≥mo se integrar√≠a este nuevo m√≥dulo en la aplicaci√≥n principal."
```

---

## FASE 2: üåê Servicio de Embeddings y Abstracci√≥n de Proveedores

**Objetivo:** Crear un servicio en KogniTerm que pueda generar embeddings utilizando diferentes proveedores (OpenAI, Gemini, Ollama) de manera abstracta.

**Tareas Clave:**
*   Crear un m√≥dulo `embeddings_service.py` con una interfaz com√∫n para generar embeddings.
*   Implementar adaptadores para al menos dos proveedores (ej., OpenAI y Gemini).
*   El servicio debe leer la configuraci√≥n del proveedor y modelo desde `config_manager`.
*   Manejar la inicializaci√≥n de clientes de API para cada proveedor.

**M√≥dulos Involucrados:**
*   `kogniterm/core/embeddings_service.py` (nuevo)
*   `kogniterm/core/llm_service.py` (posible extensi√≥n o referencia)
*   `kogniterm/terminal/config_manager.py` (usado para leer configuraci√≥n)

**Prompt para el Orquestador de Kilo Code (Fase 2):**

```
"Desarrolla la Fase 2 del sistema RAG de codebase para KogniTerm. El objetivo es crear un `Embeddings Service` abstracto. Crea un nuevo m√≥dulo `kogniterm/core/embeddings_service.py` que proporcione una interfaz unificada para generar embeddings.

Este servicio debe ser capaz de:
1.  Leer la configuraci√≥n del proveedor de embeddings (ej., 'openai', 'gemini', 'ollama') y el modelo (ej., 'text-embedding-ada-002', 'embedding-001') a trav√©s del `config_manager` implementado en la Fase 1.
2.  Implementar adaptadores para al menos OpenAI y Google Gemini, manejando la inicializaci√≥n de sus respectivos clientes de API (ej., `openai.OpenAI()`, `google.generativeai.GenerativeModel()`).
3.  La funci√≥n principal `generate_embeddings(text: list[str]) -> list[list[float]]` debe devolver una lista de vectores de embeddings para una lista de textos de entrada.
4.  Aseg√∫rate de manejar posibles errores de conexi√≥n o autenticaci√≥n de forma elegante."
```

---

## FASE 3: üìö Indexador de Codebase (Chunking y Embedding)

**Objetivo:** Implementar la l√≥gica para recorrer un directorio, dividir archivos de c√≥digo en "chunks" y generar embeddings para cada uno.

**Tareas Clave:**
*   Crear un m√≥dulo `codebase_indexer.py`.
*   Funci√≥n para listar archivos de c√≥digo en un directorio, respetando exclusiones configuradas (desde `config_manager`).
*   Implementar una estrategia de "chunking" b√°sica para archivos de texto/c√≥digo (ej., dividir por l√≠neas con un tama√±o m√°ximo, o por delimitadores simples como funciones/clases si es un archivo Python).
*   Utilizar el `Embeddings Service` (de la Fase 2) para generar embeddings para cada chunk.
*   Estructurar los datos del chunk (contenido, ruta del archivo, l√≠neas de inicio/fin) para su almacenamiento.

**M√≥dulos Involucrados:**
*   `kogniterm/core/context/codebase_indexer.py` (nuevo)
*   `kogniterm/core/embeddings_service.py` (usado)
*   `kogniterm/terminal/config_manager.py` (usado para exclusiones)

**Prompt para el Orquestador de Kilo Code (Fase 3):**

```
"Procede con la Fase 3 del sistema RAG de codebase para KogniTerm. El objetivo es desarrollar el `Codebase Indexer`. Crea un nuevo m√≥dulo `kogniterm/core/context/codebase_indexer.py`.

Este m√≥dulo debe incluir:
1.  Una funci√≥n `list_code_files(project_path: str) -> list[str]` que recorra el `project_path` y devuelva una lista de rutas de archivos de c√≥digo. Debe respetar las listas de exclusi√≥n de directorios y tipos de archivos configuradas a trav√©s del `config_manager` (ej., `node_modules`, `.git`, `__pycache__`).
2.  Una funci√≥n `chunk_file(file_path: str) -> list[dict]` que lea un archivo de c√≥digo y lo divida en 'chunks' l√≥gicos. Cada chunk debe ser un diccionario con `{'content': '...', 'file_path': '...', 'start_line': ..., 'end_line': ...}`. Para empezar, una estrategia simple de divisi√≥n por l√≠neas o p√°rrafos es suficiente.
3.  Una funci√≥n `index_project(project_path: str)` que orqueste el proceso: liste archivos, los divida en chunks y, para cada chunk, genere su embedding utilizando el `Embeddings Service` desarrollado en la Fase 2. El output de esta funci√≥n debe ser una lista de diccionarios, donde cada diccionario contenga el chunk y su embedding."
```

---

## FASE 4: üìä Gesti√≥n de la Base de Datos Vectorial (ChromaDB)

**Objetivo:** Integrar ChromaDB en modo persistente para almacenar los embeddings y metadatos por cada proyecto.

**Tareas Clave:**
*   Crear un m√≥dulo `vector_db_manager.py` que abstraiga la interacci√≥n con ChromaDB.
*   Funciones para inicializar una colecci√≥n de ChromaDB en una ruta de directorio espec√≠fica (ej., `.kogniterm/vector_db/`).
*   Funciones para a√±adir chunks (contenido, metadatos, embeddings) a la base de datos.
*   Funciones para buscar los K vecinos m√°s cercanos (chunks) dada una consulta de embedding.
*   Asegurar que cada proyecto tenga su propia base de datos aislada.

**M√≥dulos Involucrados:**
*   `kogniterm/core/context/vector_db_manager.py` (nuevo)
*   `kogniterm/core/context/codebase_indexer.py` (usado para obtener chunks y embeddings)

**Prompt para el Orquestador de Kilo Code (Fase 4):**

```
"Contin√∫a con la Fase 4 del sistema RAG de codebase para KogniTerm. El objetivo es implementar la gesti√≥n de la Base de Datos Vectorial utilizando ChromaDB en modo persistente. Crea un nuevo m√≥dulo `kogniterm/core/context/vector_db_manager.py`.

Este m√≥dulo debe proporcionar una clase `VectorDBManager` con los siguientes m√©todos:
1.  `__init__(self, project_path: str)`: Inicializa la instancia de ChromaDB para el `project_path` dado. La base de datos debe persistir en una subcarpeta oculta como `.kogniterm/vector_db/` dentro del directorio del proyecto.
2.  `add_chunks(self, chunks: list[dict], embeddings: list[list[float]])`: Recibe una lista de diccionarios de chunks (con `content`, `file_path`, etc.) y sus correspondientes embeddings. Debe a√±adir estos datos a la colecci√≥n de ChromaDB.
3.  `search(self, query_embedding: list[float], k: int = 5) -> list[dict]`: Realiza una b√∫squeda de similitud en la base de datos con un `query_embedding` y devuelve los `k` chunks m√°s relevantes como diccionarios (incluyendo `content` y `metadatos`).

Aseg√∫rate de que cada instancia de `VectorDBManager` sea completamente independiente para cada `project_path`, garantizando el aislamiento de las bases de datos vectoriales entre proyectos."
```

---

## FASE 5: üîç Herramienta de Recuperaci√≥n (Retriever Tool)

**Objetivo:** Desarrollar una herramienta que el agente pueda usar para buscar fragmentos de c√≥digo relevantes en la Base de Datos Vectorial.

**Tareas Clave:**
*   Crear una nueva herramienta `codebase_search_tool.py`.
*   La herramienta debe tomar una consulta de texto como entrada.
*   Utilizar el `Embeddings Service` (de la Fase 2) para generar un embedding de la consulta.
*   Utilizar el `VectorDBManager` (de la Fase 4) para buscar los chunks m√°s relevantes.
*   Formatear los resultados de la b√∫squeda para que sean √∫tiles como contexto para el LLM.

**M√≥dulos Involucrados:**
*   `kogniterm/core/tools/codebase_search_tool.py` (nuevo)
*   `kogniterm/core/embeddings_service.py` (usado)
*   `kogniterm/core/context/vector_db_manager.py` (usado)

**Prompt para el Orquestador de Kilo Code (Fase 5):**

```
"Implementa la Fase 5 del sistema RAG de codebase para KogniTerm. El objetivo es crear una nueva herramienta llamada `codebase_search_tool.py` dentro de `kogniterm/core/tools/`.

Esta herramienta debe ser una funci√≥n (o clase de herramienta) que:
1.  Acepte un argumento `query: str` (la consulta de b√∫squeda del agente) y `k: int` (n√∫mero de resultados a devolver).
2.  Utilice el `Embeddings Service` (de la Fase 2) para generar un embedding vectorial de la `query`.
3.  Inicialice o acceda a una instancia del `VectorDBManager` (de la Fase 4) para el proyecto actual.
4.  Use el m√©todo `search` del `VectorDBManager` con el embedding de la consulta para recuperar los `k` chunks de c√≥digo m√°s relevantes.
5.  Formatee los resultados de la b√∫squeda (ej., concatenando el contenido de los chunks con su `file_path` y `start_line`) en una cadena de texto clara que pueda ser f√°cilmente incorporada al contexto de un LLM.
6.  La herramienta debe devolver esta cadena formateada como su resultado."
```

---

## FASE 6: üîÑ Integraci√≥n y Flujo de Trabajo RAG

**Objetivo:** Integrar todas las fases anteriores en el flujo principal de KogniTerm y habilitar la indexaci√≥n autom√°tica/manual y el uso del RAG por el agente.

**Tareas Clave:**
*   Modificar el bucle principal o el `AgentState` para detectar si un proyecto ha sido indexado y ofrecer/realizar la indexaci√≥n inicial.
*   Implementar un mecanismo para la actualizaci√≥n incremental del √≠ndice (ej., al guardar archivos o con un comando `kogniterm index refresh`).
*   Modificar la l√≥gica del `llm_service` o del `agent_state` para que, antes de hacer una llamada al LLM para ciertas tareas, considere usar la `codebase_search_tool` para enriquecer el contexto.
*   Asegurar que los resultados de la `codebase_search_tool` se inyecten de forma estructurada y √∫til en el `SystemMessage` o `UserMessage` para el LLM.

**M√≥dulos Involucrados:**
*   `kogniterm/main.py`
*   `kogniterm/core/agent_state.py`
*   `kogniterm/core/llm_service.py`
*   `kogniterm/terminal/config_manager.py`
*   `kogniterm/core/context/codebase_indexer.py`
*   `kogniterm/core/context/vector_db_manager.py`
*   `kogniterm/core/tools/codebase_search_tool.py`

**Prompt para el Orquestador de Kilo Code (Fase 6):**

```
"Finaliza la implementaci√≥n del sistema RAG de codebase para KogniTerm con la Fase 6: Integraci√≥n y Flujo de Trabajo RAG. El objetivo es integrar todos los componentes desarrollados en las fases anteriores en el flujo principal de KogniTerm.

Necesitas:
1.  Modificar `kogniterm/main.py` o `kogniterm/core/agent_state.py` para:
    *   Detectar si el proyecto actual tiene un √≠ndice de codebase (`.kogniterm/vector_db/`).
    *   Si no lo tiene, preguntar al usuario si desea indexarlo o indexarlo autom√°ticamente si la configuraci√≥n lo permite, utilizando el `codebase_indexer.py`.
    *   Implementar un comando CLI `kogniterm index refresh` para reindexar el proyecto manualmente.
2.  Modificar `kogniterm/core/llm_service.py` o la l√≥gica de toma de decisiones del agente (si existe un m√≥dulo espec√≠fico para ello) para:
    *   Identificar cu√°ndo una consulta del usuario o una tarea del agente podr√≠a beneficiarse de la recuperaci√≥n de c√≥digo.
    *   En esos casos, el agente debe invocar la `codebase_search_tool` (de la Fase 5) con una consulta relevante.
    *   Inyectar los resultados de la b√∫squeda de la herramienta (`codebase_search_tool`) en el contexto del LLM (ej., como parte del `SystemMessage` o como un `ToolMessage` espec√≠fico) antes de generar la respuesta final.
3.  Asegurarse de que el flujo general del agente sea coherente y que el contexto de c√≥digo recuperado mejore la calidad de las respuestas del LLM sin sobrecargarlo."
```
