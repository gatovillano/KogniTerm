# üìù Documentaci√≥n del M√≥dulo `HistoryManager` en KogniTerm

Este documento detalla el funcionamiento interno y la arquitectura de la clase `HistoryManager`, ubicada en `kogniterm/core/history_manager.py`, responsable de la gesti√≥n del historial de conversaci√≥n en KogniTerm.

## üéØ 1. Prop√≥sito General

La clase `HistoryManager` est√° dise√±ada para gestionar de manera eficiente y robusta el historial de conversaci√≥n entre el usuario y el asistente de IA. Su objetivo principal es:
*   Almacenar, cargar y guardar mensajes de conversaci√≥n.
*   Mantener el historial dentro de l√≠mites configurables (n√∫mero de mensajes y longitud en caracteres).
*   Optimizar el rendimiento mediante el uso de cach√©.
*   Garantizar la integridad de los mensajes y manejar posibles errores durante la serializaci√≥n/deserializaci√≥n.

## üõ†Ô∏è 2. Atributos Principales

| Atributo                      | Tipo                  | Descripci√≥n                                                                                                                                                                                                           |
| :---------------------------- | :-------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `history_file_path`           | `str`                 | Ruta absoluta o relativa al archivo JSON donde se persiste el historial de conversaci√≥n.                                                                                                                              |
| `max_history_messages`        | `int`                 | N√∫mero m√°ximo de mensajes que se deben mantener en el historial activo. Los mensajes m√°s antiguos se truncan si se excede este l√≠mite.                                                                                 |
| `max_history_chars`           | `int`                 | Longitud m√°xima en caracteres (aproximadamente) del historial de conversaci√≥n. Se utiliza junto con `max_history_messages` para truncar el historial.                                                                     |
| `conversation_history`        | `List[BaseMessage]`   | Lista de objetos `BaseMessage` (de Langchain) que representan el historial de conversaci√≥n actual en memoria.                                                                                                         |
| `tokenizer`                   | `tiktoken.Encoding`   | Instancia del tokenizador `tiktoken` (modelo `gpt-4`) utilizado para calcular la longitud de los mensajes en tokens.                                                                                                   |
| `_message_length_cache`       | `Dict[int, int]`      | Cach√© para almacenar la longitud calculada de los mensajes (en caracteres JSON) utilizando un hash del mensaje como clave. Esto evita rec√°lculos redundantes.                                                            |

### Constantes de Configuraci√≥n

*   `MIN_MESSAGES_TO_KEEP`: M√≠nimo de mensajes a mantener incluso despu√©s de truncamiento (por defecto 5).
*   `MAX_SUMMARY_LENGTH_RATIO`: Proporci√≥n del `max_history_chars` dedicada al resumen (25%).
*   `DEFAULT_MAX_SUMMARY_LENGTH`: Longitud m√°xima por defecto para un resumen (2000 caracteres).
*   `SUMMARY_TRUNCATION_SUFFIX`: Sufijo a√±adido a los res√∫menes truncados.
*   `MAX_TOOL_MESSAGE_CONTENT_LENGTH_ASSUMED`: Longitud m√°xima asumida para el contenido de un `ToolMessage`.

## ‚öôÔ∏è 3. M√©todos Clave

### 3.1. M√©todos Privados (Auxiliares)

*   `_get_token_count(self, text: str) -> int`:
    *   Calcula el n√∫mero de tokens para una cadena de texto dada usando el tokenizador `tiktoken`.
*   `_get_message_hash(self, message: BaseMessage) -> int`:
    *   Genera un hash √∫nico para un mensaje `BaseMessage` bas√°ndose en su contenido y `tool_calls` (si existen). Utilizado para la cach√©.
*   `_get_message_length(self, message: BaseMessage) -> int`:
    *   Calcula la longitud de un mensaje (en caracteres de su representaci√≥n JSON) utilizando la cach√©. Si el mensaje no est√° en cach√©, lo serializa y almacena su longitud.
*   `_to_litellm_message_for_len_calc(self, message: BaseMessage) -> Dict[str, Any]`:
    *   Convierte un mensaje de Langchain (`BaseMessage`) a un formato compatible con LiteLLM para calcular su longitud. Esto es necesario porque `json.dumps` se aplica a este formato para obtener la longitud de serializaci√≥n.
*   `_load_history(self) -> List[BaseMessage]`:
    *   **Prop√≥sito**: Carga el historial de conversaci√≥n desde el archivo JSON especificado en `history_file_path`.
    *   **Funcionamiento**:
        *   Verifica si la ruta del archivo existe y si el archivo no est√° vac√≠o.
        *   Lee y decodifica el JSON del archivo.
        *   Itera sobre los elementos serializados y los convierte de nuevo a objetos `BaseMessage` (HumanMessage, AIMessage, ToolMessage, SystemMessage) de Langchain.
        *   Maneja la deserializaci√≥n de `tool_calls` dentro de `AIMessage`, incluyendo la l√≥gica para parsear argumentos que podr√≠an estar como cadenas JSON.
        *   Incluye manejo de errores para `json.JSONDecodeError` y otras excepciones.
    *   **Retorna**: Una lista de objetos `BaseMessage` o una lista vac√≠a si hay errores o el archivo est√° vac√≠o/no existe.
*   `_save_history(self, history: List[BaseMessage])`:
    *   **Prop√≥sito**: Persiste el historial de conversaci√≥n actual en el archivo JSON.
    *   **Funcionamiento**:
        *   Crea el directorio si no existe.
        *   Convierte la lista de objetos `BaseMessage` a una lista de diccionarios serializables, manejando la estructura espec√≠fica de `AIMessage` con `tool_calls`.
        *   Escribe la representaci√≥n JSON formateada (con indentaci√≥n) en el archivo.
        *   Actualiza `self.conversation_history` in-place si la lista proporcionada es diferente para mantener las referencias.
*   `_filter_empty_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]`:
    *   Filtra mensajes de asistente que est√°n vac√≠os y no tienen `tool_calls`. Utilizado durante el truncamiento o resumen.
*   `_truncate_history_by_length(self, history: List[BaseMessage], max_length: int) -> List[BaseMessage]`:
    *   Trunca el historial desde el principio (mensajes m√°s antiguos) para asegurar que la longitud total (en caracteres JSON) no exceda `max_length`. Siempre intenta mantener `MIN_MESSAGES_TO_KEEP`.
*   `_summarize_and_truncate_history(self, history: List[BaseMessage]) -> List[BaseMessage]`:
    *   **Prop√≥sito**: Genera un resumen del historial m√°s antiguo y lo concatena con los mensajes m√°s recientes para mantener el historial dentro de los l√≠mites.
    *   **Funcionamiento**:
        *   Identifica los mensajes que deben ser resumidos y los mensajes m√°s recientes que deben conservarse.
        *   Utiliza un LLM (a trav√©s de `llm_service.get_llm_model().invoke`) para generar un resumen de los mensajes antiguos.
        *   Si el resumen excede la longitud m√°xima permitida, lo trunca y a√±ade un sufijo.
        *   Reemplaza los mensajes antiguos con el `SystemMessage` de resumen.
        *   Este m√©todo es clave para mantener un contexto relevante sin exceder los l√≠mites del modelo.
*   `_to_litellm_message(self, message: BaseMessage) -> Dict[str, Any]`:
    *   Convierte un mensaje de Langchain a un formato compatible con LiteLLM, que es utilizado internamente para la comunicaci√≥n con el modelo de lenguaje.
*   `_get_current_history_length(self) -> int`:
    *   Calcula la longitud total del historial actual en caracteres JSON, utilizando la cach√© de longitud de mensajes.
*   `_validate_and_get_history(self, current_history: List[BaseMessage], messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]`:
    *   Valida el historial para asegurar la integridad de los pares `AIMessage` y `ToolMessage`.
    *   Si un `ToolMessage` no tiene un `tool_call_id` que corresponda a un `AIMessage` previo con `tool_calls`, lo marca como inv√°lido y lo ignora.
*   `_remove_invalid_tool_messages(self, messages: List[BaseMessage]) -> List[BaseMessage]`:
    *   Elimina `ToolMessage` inv√°lidos (aquellos sin un `tool_call_id` correspondiente a un `AIMessage` previo).
*   `_handle_history_truncation(self, current_history: List[BaseMessage]) -> List[BaseMessage]`:
    *   Aplica la l√≥gica de truncamiento y resumen al historial, combinando `_truncate_history_by_length` y `_summarize_and_truncate_history` para mantener el historial dentro de `max_history_messages` y `max_history_chars`.

### 3.2. M√©todos P√∫blicos

*   `add_message(self, message: BaseMessage)`:
    *   **Prop√≥sito**: Agrega un nuevo mensaje (`BaseMessage`) al historial de conversaci√≥n en memoria y luego persiste el historial actualizado en el archivo JSON.
*   `get_history(self) -> List[BaseMessage]`:
    *   **Prop√≥sito**: Retorna la lista actual de mensajes del historial de conversaci√≥n en memoria.
*   `clear_history(self)`:
    *   **Prop√≥sito**: Limpia completamente el historial de conversaci√≥n en memoria y tambi√©n borra el historial persistido en el archivo JSON. Tambi√©n vac√≠a la cach√© de longitud de mensajes.
*   `get_formatted_history(self, prompt_messages: Optional[List[BaseMessage]] = None) -> List[Dict[str, Any]]`:
    *   **Prop√≥sito**: Prepara y formatea el historial de conversaci√≥n para ser enviado a un modelo de lenguaje.
    *   **Funcionamiento**:
        *   Combina el historial interno con `prompt_messages` opcionales.
        *   Aplica truncamiento y resumen para asegurar que el historial se ajuste a los l√≠mites configurados (`max_history_messages`, `max_history_chars`).
        *   Valida la integridad de los `ToolMessage` y los elimina si son inv√°lidos.
        *   Convierte los mensajes de Langchain a un formato compatible con LiteLLM/OpenAI (`Dict[str, Any]`).
        *   Asegura que el historial final no exceda los l√≠mites de tokens del modelo.

## üîÑ 4. Flujo de Trabajo del Historial

1.  **Inicializaci√≥n**: Al crear una instancia de `HistoryManager`, se intenta cargar el historial desde `history_file_path`. Si el archivo no existe o est√° vac√≠o/corrupto, se inicializa un historial vac√≠o.
2.  **A√±adir Mensajes**: Cuando se llama a `add_message()`, el nuevo `BaseMessage` se a√±ade a `conversation_history` y el historial completo se guarda inmediatamente en el archivo.
3.  **Obtener Historial Formateado**: Antes de interactuar con un LLM, `get_formatted_history()` se encarga de:
    *   Unir el historial en memoria con cualquier mensaje de `prompt_messages` que el usuario desee a√±adir para la invocaci√≥n actual.
    *   Aplicar la l√≥gica de truncamiento y resumen para asegurar que el historial final no exceda los l√≠mites de tama√±o y cantidad de mensajes.
    *   Validar y limpiar los `ToolMessage` para evitar inconsistencias.
    *   Convertir los mensajes al formato `LiteLLM` (`Dict[str, Any]`) esperado por el modelo de lenguaje.
4.  **Persistencia**: El historial se guarda en formato JSON en el disco (`.json`) para mantener la continuidad de la conversaci√≥n entre sesiones.

## ‚ö° 5. Optimizaci√≥n y Robustez

*   **Cach√© de Longitud**: El uso de `_message_length_cache` evita rec√°lculos costosos de la longitud de los mensajes, mejorando el rendimiento al truncar y resumir.
*   **Manejo de Errores**: Se implementan bloques `try-except` para gestionar errores durante la carga y guardado del historial (ej. `json.JSONDecodeError`), lo que hace que el sistema sea m√°s resistente a archivos de historial corruptos.
*   **Validaci√≥n de `ToolMessage`**: La l√≥gica para validar `ToolMessage` asegura que solo los mensajes de herramientas v√°lidamente vinculados a un `AIMessage` previo con `tool_calls` sean incluidos en el historial final, previniendo errores en el LLM.
*   **Truncamiento Inteligente**: La combinaci√≥n de truncamiento por n√∫mero de mensajes y por longitud en caracteres, junto con la capacidad de resumir mensajes antiguos, permite mantener un historial relevante y conciso sin sobrecargar el modelo de lenguaje.
*   **Persistencia en JSON**: El formato JSON es legible y f√°cil de depurar, adem√°s de ser un est√°ndar para la serializaci√≥n de datos.
*   **Uso de `tiktoken`**: Para un c√°lculo preciso de tokens, lo cual es crucial para interactuar con modelos de lenguaje que tienen l√≠mites de contexto basados en tokens.
